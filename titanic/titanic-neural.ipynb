{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "print(os.listdir(\"./data\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b27abfb1-115f-48fb-9b15-bdb4220adc4a",
    "_uuid": "01d5ae8d1a2420ca7b4a1eff743316e19e39d2d1"
   },
   "outputs": [],
   "source": [
    "# Next Steps\n",
    "# - Step I\n",
    "# - Analyse hyperparam tuning results and fine tune hyper params\n",
    "# - Submit predictions\n",
    "# - Step II\n",
    "# - Incorporate emabarked, cabin, name\n",
    "# - Submit improved predictions\n",
    "# - Step III\n",
    "# - Have preprocessing as a hyperparam\n",
    "# - Submit imporoved predictions\n",
    "# - Step IV\n",
    "# - Error Analysis and Hand Engg Features\n",
    "# - Submit Imporved Solution\n",
    "# - Step V\n",
    "# - Cleanup, Save to Github, Extract Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Orig Shape: (891, 12)\n",
      "Test Data Orig Shape: (418, 11)\n"
     ]
    }
   ],
   "source": [
    "# Read files\n",
    "def read_data():\n",
    "    data_dir = './data'\n",
    "\n",
    "    train_path = os.path.join(data_dir, 'train.csv')\n",
    "    train_orig = pd.read_csv(train_path)\n",
    "    print('Train Data Orig Shape:', train_orig.shape)\n",
    "\n",
    "    test_path = os.path.join(data_dir, 'test.csv')\n",
    "    test_orig = pd.read_csv(test_path)\n",
    "    print('Test Data Orig Shape:', test_orig.shape)\n",
    "    \n",
    "    return train_orig, test_orig\n",
    "\n",
    "if True:\n",
    "    train_orig, test_orig = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "ff75381b-a8e5-4839-838d-1db769bcafd7",
    "_uuid": "503b864ba6354de772b555701f7e72cbf64aed31",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare Dev Set\n",
    "def split_data(data, split_ratio=0.9):\n",
    "#     data = data.sample(frac=1)\n",
    "    \n",
    "#     train_data = data.iloc[:int(data.shape[0]*split_ratio),:]\n",
    "#     print('Train Data Shape:', train_data.shape)\n",
    "\n",
    "#     dev_data = data.iloc[train_data.shape[0]:,:]\n",
    "#     print('Dev Data Shape:', dev_data.shape)\n",
    "    \n",
    "#     assert len(dev_data.index.intersection(train_data.index)) == 0\n",
    "#     return train_data, dev_data\n",
    "    train_test_split()\n",
    "    \n",
    "if False:\n",
    "    train_data, dev_data = train_test_split(train_orig, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _x = {}\n",
    "# for name in train_orig['Name'].unique():\n",
    "#     for part in name.split():\n",
    "#         cleaned = re.sub('[,()\\\"\\'\\.]', '', part).lower()\n",
    "#         if cleaned not in _x:\n",
    "#             _x[cleaned] = 1\n",
    "#         else:\n",
    "#             _x[cleaned] += 1\n",
    "# _a = [(_z, _x[_z]) for _z in sorted(list(_x.keys()), key=lambda _y: _x[_y], reverse=True) if _x[_z] > 5]\n",
    "# len(_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "c8cdb1d4-56a5-4a99-b3c9-2eb843478e07",
    "_uuid": "2acac90d3aba6531e6460e70a9b6a88421d18123"
   },
   "outputs": [],
   "source": [
    "def preprocess_basic(data, norms=None, test=False):\n",
    "    ageMean = ageStd = fareMean = fareStd = None\n",
    "    if norms is not None:\n",
    "        ageMean, ageStd, fareMean, fareStd = norms\n",
    "    \n",
    "    temp = data.copy()\n",
    "    \n",
    "    temp['Pclass'] -= 2\n",
    "    \n",
    "    temp['Sex'] = (temp['Sex'].astype('category').cat.codes*2) -1\n",
    "    \n",
    "    if ageMean is None:\n",
    "        ageMean = temp['Age'].mean()\n",
    "    if ageStd is None:\n",
    "        ageStd = temp['Age'].std()\n",
    "    temp.loc[temp['Age'].isnull(), 'Age'] = ageMean\n",
    "    temp['Age'] = (temp['Age'] - ageMean) / ageStd\n",
    "    \n",
    "    temp['SibSp'] = (temp['Parch'] / 4) - 1\n",
    "    temp['Parch'] = (temp['Parch'] / 3) - 1 \n",
    "    \n",
    "    if fareMean is None:\n",
    "        fareMean = temp['Fare'].mean()\n",
    "    if fareStd is None:\n",
    "        fareStd = temp['Fare'].std()\n",
    "    temp.loc[temp['Fare'].isnull(), 'Fare'] = fareMean\n",
    "    temp['Fare'] = (temp['Fare'] - fareMean) / fareStd\n",
    "    \n",
    "    temp.loc[temp['Embarked'].isnull(), 'Embarked'] = 'S'\n",
    "    temp['Embarked'] = temp['Embarked'].astype('category').cat.codes - 1\n",
    "    \n",
    "    cols = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "    if not test:\n",
    "        cols.append('Survived')\n",
    "    temp = temp[cols]\n",
    "    \n",
    "    norms = (ageMean, ageStd, fareMean, fareStd)\n",
    "    return temp, norms\n",
    "\n",
    "def preprocess_pclass(data, config):\n",
    "    if config['pclass_is_ordinal']:\n",
    "        data['Pclass'] -= 2\n",
    "    else:\n",
    "        for pclass in [1, 2, 3]:\n",
    "            data['Pclass_' + str(pclass)] = np.where(data['Pclass'] == pclass, 1, -1)\n",
    "        data = data.drop(columns=['Pclass'])\n",
    "    return data\n",
    "\n",
    "def preprocess_embarked(data, config):\n",
    "    if config['embarked_is_ordinal']:\n",
    "        data.loc[data['Embarked'].isnull(), 'Embarked'] = 'S'\n",
    "        data['Embarked'] = data['Embarked'].astype('category').cat.codes - 1\n",
    "    else:\n",
    "        for emb in ['S', 'C', 'Q']:\n",
    "            data['Embarked_' + emb] = np.where(data['Embarked'] == emb, 1, -1)\n",
    "        data = data.drop(columns=['Embarked'])\n",
    "    return data\n",
    "\n",
    "def preprocess_cabin(data, config):\n",
    "    for cab in ['C', 'G', 'T', 'A', 'B', 'F', 'D', 'E']:\n",
    "        data['Cabin_' + cab] = np.where(data['Cabin'].apply(lambda c: c[0] if type(c) is str else None) == cab, 1, -1)\n",
    "    data = data.drop(columns=['Cabin'])\n",
    "    return data\n",
    "\n",
    "def preprocess_names(data, config, names):\n",
    "    if names is None:\n",
    "        names = {}\n",
    "        for name in data['Name']:\n",
    "            for part in name.split():\n",
    "                cleaned = re.sub('[,()\\\"\\'\\.]', '', part).lower()\n",
    "                if cleaned not in names:\n",
    "                    names[cleaned] = 1\n",
    "                else:\n",
    "                    names[cleaned] += 1\n",
    "        names = [nm for nm in sorted(list(names.keys()), key=lambda n: names[n], reverse=True) if names[nm] >= config['name_top_common']]\n",
    "    for name in names:\n",
    "        data['Name_' + name] = np.where(data['Name'] == name, 1, -1)\n",
    "    data = data.drop(columns=['Name'])\n",
    "    return data, names\n",
    "\n",
    "default_preprocess_config = {\n",
    "    'pclass_is_ordinal': False,\n",
    "    'embarked_is_ordinal': False,\n",
    "    'name_process_method': 'top-common-names',\n",
    "    'name_top_common': 5\n",
    "}\n",
    "\n",
    "def preprocess_advanced(data, computed=None, config=default_preprocess_config):\n",
    "    \n",
    "    ageMean = ageStd = fareMean = fareStd = None\n",
    "    if computed is not None:\n",
    "        norms, names = computed\n",
    "        ageMean, ageStd, fareMean, fareStd = norms\n",
    "    \n",
    "    temp = data.copy()\n",
    "    \n",
    "    # Pclass\n",
    "    temp = preprocess_pclass(temp, config)\n",
    "    \n",
    "    # Sex\n",
    "    temp['Sex'] = (temp['Sex'].astype('category').cat.codes*2) -1\n",
    "    \n",
    "    # Age - Fill missing and normalize\n",
    "    if ageMean is None:\n",
    "        ageMean = temp['Age'].mean()\n",
    "    if ageStd is None:\n",
    "        ageStd = temp['Age'].std()\n",
    "    temp.loc[temp['Age'].isnull(), 'Age'] = ageMean\n",
    "    temp['Age'] = (temp['Age'] - ageMean) / ageStd\n",
    "    \n",
    "    # Sibsp and Parch normalize\n",
    "    temp['SibSp'] = (temp['Parch'] / 4) - 1\n",
    "    temp['Parch'] = (temp['Parch'] / 3) - 1 \n",
    "    \n",
    "    # Fare - Fill missing and normalize\n",
    "    if fareMean is None:\n",
    "        fareMean = temp['Fare'].mean()\n",
    "    if fareStd is None:\n",
    "        fareStd = temp['Fare'].std()\n",
    "    temp.loc[temp['Fare'].isnull(), 'Fare'] = fareMean\n",
    "    temp['Fare'] = (temp['Fare'] - fareMean) / fareStd\n",
    "    \n",
    "    # Embarked\n",
    "    temp = preprocess_embarked(temp, config)\n",
    "    \n",
    "    # Cabin\n",
    "    temp = preprocess_cabin(temp, config)\n",
    "    \n",
    "    # Names\n",
    "    temp, names = preprocess_names(temp, config, names if computed is not None else None)\n",
    "    \n",
    "    drop_cols = ['PassengerId', 'Ticket']\n",
    "    temp = temp.drop(columns=drop_cols)\n",
    "    \n",
    "    norms = (ageMean, ageStd, fareMean, fareStd)\n",
    "    computed = norms, names\n",
    "    return temp, computed\n",
    "    \n",
    "preprocess = preprocess_basic\n",
    "\n",
    "if False:\n",
    "    train_data, norms = preprocess(train_data)\n",
    "    dev_data, _ = preprocess(dev_data, norms)\n",
    "    test_data, _ = preprocess(test_orig, norms, test=True)\n",
    "if False:\n",
    "    train_data, norms = preprocess_advanced(train_data)\n",
    "    dev_data, _ = preprocess_advanced(dev_data, norms)\n",
    "    test_data, _ = preprocess_advanced(test_orig, norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "3aecd557-9d12-4e77-9232-98dc5437d7d4",
    "_uuid": "412e9fa2672d317c725e54bc6a92ae8922ccd846"
   },
   "outputs": [],
   "source": [
    "# Plot train data\n",
    "# %pylab inline\n",
    "# pylab.rcParams['figure.figsize'] = (20, 6)\n",
    "# if False:\n",
    "#     x = 'Fare'\n",
    "#     y = 'Age'\n",
    "#     survived_data = train_data[train_data['Survived'] == 1]\n",
    "#     plt.scatter(survived_data[x],survived_data[y], color='green', marker='o')\n",
    "#     not_survived_data = train_data[train_data['Survived'] == 0]\n",
    "#     plt.scatter(not_survived_data[x],not_survived_data[y], color='red', marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "f93b44de-d6c4-4c34-91a3-2f1dc995ef00",
    "_uuid": "dbc7925482c80503ea27b1cdc100539c1e1bcb86"
   },
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    X = data.drop(['Survived'], axis=1).as_matrix().T\n",
    "    Y = data['Survived'].as_matrix().reshape(-1, 1).T\n",
    "#     print('Shape X: {}, Y: {}'.format(X.shape, Y.shape))\n",
    "    return X, Y\n",
    "if False:\n",
    "    X, Y = prepare_data(train_data)\n",
    "    X_dev, Y_dev = prepare_data(dev_data)\n",
    "    X_test = test_data.as_matrix().T\n",
    "    print('Shape X test: {}'.format(X_test.shape))\n",
    "    n, m = X.shape\n",
    "    m_dev = X_dev.shape[1]\n",
    "    m_test = X_test.shape[1]\n",
    "    print('m:{}, n:{}, m_dev:{}, m_test:{}'.format(m, n, m_dev, m_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "66bd73c8-957d-4281-b7c7-d208a706feb8",
    "_uuid": "180833ec8fabaef1786e1179bd7aa64128944ac1"
   },
   "outputs": [],
   "source": [
    "def get_model_config(**kwargs):\n",
    "    config = {\n",
    "        'activation_fn': 'relu',\n",
    "        'cost_fn': 'log_cost',\n",
    "        'layer_dims': [],\n",
    "        'lrate': 0.1,\n",
    "        'niters': 300,\n",
    "        'reg_fn': 'l2', # can be None\n",
    "        'reg_factor': 0.1\n",
    "    }\n",
    "    return dict(config, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "ac32dc9a-53d8-43f4-a8e9-77afe115a424",
    "_uuid": "4b6f18403e9d767fe4c26c69afcdaa71b6db1a28"
   },
   "outputs": [],
   "source": [
    "class Weights:\n",
    "    def __init__(self, nprev, ncurr):\n",
    "        self.nprev = nprev\n",
    "        self.ncurr = ncurr\n",
    "        self.W = np.random.randn(ncurr, nprev) * np.sqrt(1/nprev)\n",
    "#         self.W = np.random.randn(ncurr, nprev) * 0.01\n",
    "        self.B = np.zeros((ncurr, 1))\n",
    "    def clone(self):\n",
    "        w = Weights(1, 1)\n",
    "        w.nprev = self.nprev\n",
    "        w.ncurr = self.ncurr\n",
    "        w.W = self.W.copy()\n",
    "        w.B = self.B.copy()\n",
    "        return w\n",
    "def init_weights(model):\n",
    "    weights = []\n",
    "    layer_dims = model['layer_dims']\n",
    "    for i in range(len(layer_dims)-1):\n",
    "        na, nb = layer_dims[i], layer_dims[i+1]\n",
    "        weights.append(Weights(na, nb))\n",
    "    return weights\n",
    "if False:\n",
    "    _layer_dims = [4,3,4,2]\n",
    "    _weights = init_weights(get_model_config(layer_dims=_layer_dims))\n",
    "    for i,w in enumerate(_weights):\n",
    "        print('W{}: '.format(i+1),w.W.shape)\n",
    "        print('B{}: '.format(i+1),w.B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "2ac19cde-f877-43e4-9238-f992f9ebdda6",
    "_uuid": "67834fbadfa9668ccc63a787e9fad85fea889d27"
   },
   "outputs": [],
   "source": [
    "# _w = Weights(3,4)\n",
    "# _wc = _w.clone()\n",
    "# _wc.W[0][0] = 1000\n",
    "# print(_w.W[0][0], _wc.W[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "83291d19-440d-40ae-94e4-0c4b919b4002",
    "_uuid": "6bf10eeee47ef9a515233762b90528482aafb6e9"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "#     print('Using Sigmoid')\n",
    "    return 1 / (1 + np.exp(-1 * z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "#     print('Using Sigmoid Derivative')\n",
    "    _exp = np.exp(-1 * z)\n",
    "    return _exp / np.square(1 + np.exp(-1 * z))\n",
    "\n",
    "def relu(z):\n",
    "#     print( 'Using Relu1')\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "#     print( 'Using Relu Derivative')\n",
    "    return np.where(z <= 0, 0, 1)\n",
    "\n",
    "def tanh(z):\n",
    "#     print( 'Using tanh')\n",
    "    exp1 = np.exp(z)\n",
    "    exp2 = np.exp(-1*z)\n",
    "    return (exp1 - exp2) / (exp1 + exp2)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "#     print( 'Using tanh derivative')\n",
    "    exp1 = np.exp(z)\n",
    "    exp2 = np.exp(-1*z)\n",
    "    return 1 - np.square((exp1 - exp2) / (exp1 + exp2))\n",
    "\n",
    "def get_activation_fn(name):\n",
    "    return {\n",
    "        'sigmoid': sigmoid,\n",
    "        'relu': relu,\n",
    "        'tanh': tanh\n",
    "    }[name]\n",
    "\n",
    "def get_activation_fn_grad(name):\n",
    "    return {\n",
    "        'sigmoid': sigmoid_derivative,\n",
    "        'relu': relu_derivative,\n",
    "        'tanh': tanh_derivative\n",
    "    }[name]\n",
    "\n",
    "def log_cost(A, Y):\n",
    "#     print( 'Using log cost')\n",
    "    m = Y.shape[1]\n",
    "#     print(Y.shape)\n",
    "    return -1 * (np.dot(np.log(A), Y.T) + np.dot(np.log(1-A), 1-Y.T)).sum() / m\n",
    "\n",
    "def log_cost_derivative(A, Y):\n",
    "#     print( 'Using log cost derivative')\n",
    "    m = Y.shape[1]\n",
    "    return ((A-Y) / (A*(1-A))) / m\n",
    "\n",
    "def mse_cost(A, Y):\n",
    "#     print( 'Using mse cost')\n",
    "    m = Y.shape[1]\n",
    "    return np.square(A - Y).sum() / (2 * m)\n",
    "\n",
    "def mse_cost_derivative(A, Y):\n",
    "#     print('Using mse cost derivative')\n",
    "    m = Y.shape[1]\n",
    "    return (A - Y).sum() / m\n",
    "\n",
    "def get_cost_fn(name):\n",
    "    return {\n",
    "        'log_cost': log_cost,\n",
    "        'mse': mse_cost \n",
    "    }[name]\n",
    "\n",
    "def get_cost_fn_grad(name):\n",
    "    return {\n",
    "        'log_cost': log_cost_derivative,\n",
    "        'mse': mse_cost_derivative \n",
    "    }[name]\n",
    "\n",
    "def l2_cost(weights, m, model):\n",
    "    W = np.concatenate((*[w.W.reshape(-1) for w in weights],), axis=0)\n",
    "#     np.concatenate((*[weightChanges[x].dW.reshape(-1) for x in range(len(weightChanges))],\n",
    "#                            *[weightChanges[x].dB.reshape(-1) for x in range(len(weightChanges))]),\n",
    "#                            axis=0)\n",
    "    return (model['reg_factor'] * np.square(W).sum()) / (2 * m)\n",
    "\n",
    "def l2_derivative(W, m, model):\n",
    "    return (model['reg_factor'] * W) / m\n",
    "\n",
    "def get_reg_cost_fn(name):\n",
    "    return {\n",
    "        None: lambda weights, m, model: 0,\n",
    "        'l2': l2_cost\n",
    "    }[name]\n",
    "\n",
    "def get_reg_grad_fn(name):\n",
    "    return {\n",
    "        None: lambda W, m, model: 0,\n",
    "        'l2': l2_derivative\n",
    "    }[name]\n",
    "\n",
    "# print('Sigmoid arr', sigmoid(np.array([-10, 0, 10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "fccd012a-1e21-4dba-9054-87ed21f15856",
    "_uuid": "86de2fed69c2c1225a7463f87accd3dc9673fc81"
   },
   "outputs": [],
   "source": [
    "class WeightChanges:\n",
    "    def __init__(self, dW, dB):\n",
    "        self.dW = dW\n",
    "        self.dB = dB\n",
    "        self.nprev = dW.shape[1]\n",
    "        self.ncurr = dW.shape[0]\n",
    "    def clone(self):\n",
    "        _temp = np.zeros((1,1))\n",
    "        w = WeightChanges(_temp, _temp)\n",
    "        w.dW = self.dW.copy()\n",
    "        w.dB = self.dB.copy()\n",
    "        w.nprev = self.nprev\n",
    "        w.ncurr = self.ncurr\n",
    "        return w\n",
    "        \n",
    "def compute_cost_and_grads(X, Y, weights, model, backprop=True): \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Feed Formward\n",
    "    Z = []\n",
    "    A = []\n",
    "    Aprev = X\n",
    "    for i, weight in enumerate(weights):\n",
    "#         print('Feed forward in layer {}'.format(i+1))\n",
    "        Z.append(np.dot(weight.W, Aprev) + weight.B)\n",
    "#         print(i < len(weights)-1)\n",
    "        curr_act = get_activation_fn(model['activation_fn'] if i < len(weights)-1 else 'sigmoid')\n",
    "        Aprev = curr_act(Z[i])\n",
    "        A.append(Aprev)\n",
    "\n",
    "    # Cost\n",
    "    Afinal = A[-1]\n",
    "    cost = get_cost_fn(model['cost_fn'])(Afinal, Y)\n",
    "    cost += get_reg_cost_fn(model['reg_fn'])(weights, m, model)\n",
    "#     print('Cost:', cost)\n",
    "    if not backprop:\n",
    "        return cost\n",
    "    \n",
    "    # Backprop\n",
    "    dAcurr = get_cost_fn_grad(model['cost_fn'])(Afinal, Y)\n",
    "    weight_changes = []\n",
    "    for i, weight in enumerate(reversed(weights)):\n",
    "        i_rev, i_norm = i, len(weights) - 1 - i\n",
    "#         print('Back in layer {}'.format(i_norm+1))\n",
    "        curr_act_grad = get_activation_fn_grad(model['activation_fn'] if i_rev > 0 else 'sigmoid')\n",
    "        dZcurr = dAcurr * curr_act_grad(Z[i_norm])\n",
    "        Aprev = A[i_norm - 1] if i_norm > 0 else X\n",
    "        dWcurr = np.dot(dZcurr, Aprev.T)\n",
    "        dWcurr += get_reg_grad_fn(model['reg_fn'])(weight.W, m, model)\n",
    "        dBcurr = dZcurr.sum(axis=1, keepdims=True)\n",
    "        dAcurr = np.dot(weight.W.T, dZcurr)\n",
    "        weight_changes.append(WeightChanges(dWcurr, dBcurr))\n",
    "    \n",
    "    return cost, list(reversed(weight_changes))\n",
    "\n",
    "if False:\n",
    "    _layer_dims = [2, 2, 1]\n",
    "    _model = get_model_config(activation_fn='relu', cost_fn='log_cost',\n",
    "                              layer_dims=_layer_dims, \n",
    "                              reg_fn='l2', reg_factor=0.1)\n",
    "    print('Model config', _model)\n",
    "    _X = np.array([1, 2, 3, 4]).reshape(2,2)\n",
    "    _Y = np.array([1, 1]).reshape(1,2)\n",
    "    _weights = init_weights(_model)\n",
    "    print('Debug X:', _X)\n",
    "    print('Debug Y:', _Y)\n",
    "    print('Debug weights W1', _weights[0].W)\n",
    "    print('Debug weights B1', _weights[0].B)\n",
    "    print('Debug weights W2', _weights[1].W)\n",
    "    print('Debug weights B2', _weights[1].B)\n",
    "    debug_cost, debug_grads = compute_cost_and_grads(_X, _Y, _weights, _model, backprop=True)\n",
    "    print('Debug cost:', debug_cost)\n",
    "    print('Debug grads dW1', debug_grads[0].dW)\n",
    "    print('Debug grads dB1', debug_grads[0].dB)\n",
    "    print('Debug grads dW2', debug_grads[1].dW)\n",
    "    print('Debug grads dB2', debug_grads[1].dB)\n",
    "# if True:\n",
    "#     debug_cost1 = compute_cost(np.array([1,2,3,4]).reshape(2,2), np.array([1, 0]).reshape(2,1), np.array([0 + 1.e-7, 1]).reshape(2,1), 1)\n",
    "#     debug_cost2 = compute_cost(np.array([1,2,3,4]).reshape(2,2), np.array([1, 0]).reshape(2,1), np.array([0 - 1.e-7, 1]).reshape(2,1), 1)\n",
    "#     print('grad numer: ', (debug_cost1 - debug_cost2) / (2.e-7))\n",
    "#     print('Debug cost', debug_cost1, debug_cost2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "e21e8a33-cc91-4fff-b0ce-462367291361",
    "_uuid": "8819b78a9964eed4b2781dd38144e720c5c06872"
   },
   "outputs": [],
   "source": [
    "# _Z1 = np.dot(_weights[0].W, _X) + _weights[0].B\n",
    "# _A1 = relu(_Z1)\n",
    "# _Z2 = np.dot(_weights[1].W, _A1) + _weights[1].B\n",
    "# # print('Z2', _Z2)\n",
    "# _A2 = sigmoid(_Z2)\n",
    "# _cost = -1 * (np.dot(np.log(_A2), _Y.T) + np.dot(np.log(1-_A2), 1-_Y.T)) / 2\n",
    "# print('Cost:', _cost)\n",
    "# _dZ2 = (_A2 - _Y) / 2\n",
    "# # print('dZ2', _dZ2)\n",
    "# _dB2 = _dZ2.sum(axis=1, keepdims=True)\n",
    "# _dB2\n",
    "# _dW2 = np.dot(_dZ2, _A1.T)\n",
    "# _dW2\n",
    "# _dA1 = np.dot(_weights[1].W.T, _dZ2)\n",
    "# _dZ1 = _dA1 * np.where(_Z1 >= 0, 1, 0)\n",
    "# _dB1 = _dZ1.sum(axis=1, keepdims=True)\n",
    "# _dB1\n",
    "# _dW1 = np.dot(_dZ1, _X.T)\n",
    "# _dW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "d3be5779-e4e1-42b5-9edd-fc5b43df99ef",
    "_uuid": "19dde3461c76f21e05677d325cc99c23c379e1fb"
   },
   "outputs": [],
   "source": [
    "def check_gradients(X, Y, weights, weightChanges, model, epsilon=1.e-7):\n",
    "    weightChangesComputed = [w.clone() for w in weightChanges]\n",
    "    for w in range(len(weights)):\n",
    "        weight = weights[w]\n",
    "        weightChangeComputed = weightChangesComputed[w]\n",
    "        for i in range(weight.W.shape[0]):\n",
    "            for j in range(weight.W.shape[1]):\n",
    "                weight_cloned = weight.clone()\n",
    "                weights[w] = weight_cloned\n",
    "                weight_cloned.W[i][j] += epsilon\n",
    "                cost_right = compute_cost_and_grads(X, Y, weights, model, backprop=False)\n",
    "                weight_cloned = weight.clone()\n",
    "                weights[w] = weight_cloned\n",
    "                weight_cloned.W[i][j] -= epsilon\n",
    "                cost_left = compute_cost_and_grads(X, Y, weights, model, backprop=False)\n",
    "                weightChangeComputed.dW[i][j] = (cost_right - cost_left) / (2 * epsilon)\n",
    "                weights[w] = weight\n",
    "        for i in range(weight.B.shape[0]):\n",
    "            weight_cloned = weight.clone()\n",
    "            weights[w] = weight_cloned\n",
    "            weight_cloned.B[i][0] += epsilon\n",
    "            cost_right = compute_cost_and_grads(X, Y, weights, model, backprop=False)\n",
    "            weight_cloned = weight.clone()\n",
    "            weights[w] = weight_cloned\n",
    "            weight_cloned.B[i][0] -= epsilon\n",
    "            cost_left = compute_cost_and_grads(X, Y, weights, model, backprop=False)\n",
    "            weightChangeComputed.dB[i][0] = (cost_right - cost_left) / (2 * epsilon)\n",
    "            weights[w] = weight\n",
    "        \n",
    "    grads = np.concatenate((*[weightChanges[x].dW.reshape(-1) for x in range(len(weightChanges))],\n",
    "                           *[weightChanges[x].dB.reshape(-1) for x in range(len(weightChanges))]),\n",
    "                           axis=0)\n",
    "    computedGrads = np.concatenate((*[weightChangesComputed[x].dW.reshape(-1) for x in range(len(weightChangesComputed))],\n",
    "                           *[weightChangesComputed[x].dB.reshape(-1) for x in range(len(weightChangesComputed))]),\n",
    "                           axis=0)\n",
    "    \n",
    "    return np.square(grads - computedGrads).sum() / (np.square(grads).sum() + np.square(computedGrads).sum())\n",
    "    \n",
    "if False:\n",
    "    l2_norm = check_gradients(_X, _Y, _weights, debug_grads, _model)\n",
    "    print('Check Gradients Score', l2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "c568faf1-9927-4fe0-91f8-ad9bb961bc83",
    "_uuid": "10243e0ab3e57358e2050cf7864e2be8c72e6e6c"
   },
   "outputs": [],
   "source": [
    "def update_params(weights, weightChanges, lrate):\n",
    "    for w in range(len(weights)):\n",
    "        weights[w].W -= weightChanges[w].dW * lrate\n",
    "        weights[w].B -= weightChanges[w].dB * lrate\n",
    "if False:\n",
    "    update_params(_weights, debug_grads, 0.01)\n",
    "    print('Updated params:', _weights[0].W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "bcc584bf-71cf-4c24-ab4b-789560c4a5da",
    "_uuid": "d68e7ce55763f59aba82d96f324407a22d8cef27",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations is 0.7354845649066162\n",
      "Gradient check score at iteration 0 is 1.6678843125859858e-07\n",
      "Cost after 10 iterations is 0.6891542379669359\n",
      "Gradient check score at iteration 10 is 6.290020728657009e-15\n",
      "Cost after 20 iterations is 0.6851896838422579\n",
      "Gradient check score at iteration 20 is 9.901032747103207e-15\n",
      "Cost after 30 iterations is 0.6822477881885847\n",
      "Gradient check score at iteration 30 is 7.120244935220291e-15\n",
      "Cost after 40 iterations is 0.680020286669299\n",
      "Gradient check score at iteration 40 is 1.4927583355336307e-14\n",
      "Cost after 50 iterations is 0.6783282164600315\n",
      "Gradient check score at iteration 50 is 2.044129716902879e-14\n",
      "Cost after 60 iterations is 0.6770362802981922\n",
      "Gradient check score at iteration 60 is 1.6334272437818108e-14\n",
      "Cost after 70 iterations is 0.6760038161060287\n",
      "Gradient check score at iteration 70 is 2.0304319047500336e-14\n",
      "Cost after 80 iterations is 0.6750964946117177\n",
      "Gradient check score at iteration 80 is 3.072005600489411e-14\n",
      "Cost after 90 iterations is 0.6742850861707758\n",
      "Gradient check score at iteration 90 is 4.014765509712116e-14\n",
      "Cost after 100 iterations is 0.6735030887556119\n",
      "Gradient check score at iteration 100 is 1.8867185171856855e-14\n",
      "Cost after 110 iterations is 0.6727372590916137\n",
      "Gradient check score at iteration 110 is 2.2593827009368724e-14\n",
      "Cost after 120 iterations is 0.6717968470106566\n",
      "Gradient check score at iteration 120 is 2.0438691578487632e-14\n",
      "Cost after 130 iterations is 0.6705688084476524\n",
      "Gradient check score at iteration 130 is 1.9386809385708412e-14\n",
      "Cost after 140 iterations is 0.6684547280426131\n",
      "Gradient check score at iteration 140 is 1.0281076945692256e-14\n",
      "Cost after 150 iterations is 0.6658591228200383\n",
      "Gradient check score at iteration 150 is 1.0060483333047068e-14\n",
      "Cost after 160 iterations is 0.6638741142663004\n",
      "Gradient check score at iteration 160 is 1.2688259054145431e-14\n",
      "Cost after 170 iterations is 0.6616680186750669\n",
      "Gradient check score at iteration 170 is 7.624914796016377e-15\n",
      "Cost after 180 iterations is 0.6591810099908143\n",
      "Gradient check score at iteration 180 is 1.0090747553298837e-14\n",
      "Cost after 190 iterations is 0.6563056149396574\n",
      "Gradient check score at iteration 190 is 9.816101863944342e-15\n",
      "Cost after 200 iterations is 0.6529086643338835\n",
      "Gradient check score at iteration 200 is 6.43231413650826e-15\n",
      "Cost after 210 iterations is 0.6489195968563638\n",
      "Gradient check score at iteration 210 is 9.04423805860178e-15\n",
      "Cost after 220 iterations is 0.6442575676353326\n",
      "Gradient check score at iteration 220 is 5.29824365913152e-15\n",
      "Cost after 230 iterations is 0.6386662245345922\n",
      "Gradient check score at iteration 230 is 5.893172309156061e-15\n",
      "Cost after 240 iterations is 0.6318202691085866\n",
      "Gradient check score at iteration 240 is 4.047395110098589e-15\n",
      "Cost after 250 iterations is 0.6237105618358529\n",
      "Gradient check score at iteration 250 is 4.7834411570946064e-15\n",
      "Cost after 260 iterations is 0.6114106641758843\n",
      "Gradient check score at iteration 260 is 1.5636013927494476e-15\n",
      "Cost after 270 iterations is 0.5965680656645366\n",
      "Gradient check score at iteration 270 is 2.159025603560062e-15\n",
      "Cost after 280 iterations is 0.5807439827000723\n",
      "Gradient check score at iteration 280 is 1.7636758473067453e-15\n",
      "Cost after 290 iterations is 0.5636556658503916\n",
      "Gradient check score at iteration 290 is 1.3938510992645453e-15\n",
      "Cost after 300 iterations is 0.5462267345694244\n",
      "Gradient check score at iteration 300 is 1.3503449338244654e-15\n",
      "Cost after 310 iterations is 0.5292535101936678\n",
      "Gradient check score at iteration 310 is 3.946186423219679e-15\n",
      "Cost after 320 iterations is 0.5139417101723337\n",
      "Gradient check score at iteration 320 is 2.1202616554191356e-15\n",
      "Cost after 330 iterations is 0.5009573963374448\n",
      "Gradient check score at iteration 330 is 3.055248215432345e-15\n",
      "Cost after 340 iterations is 0.4903569453953881\n",
      "Gradient check score at iteration 340 is 1.4548309773178779e-15\n",
      "Cost after 350 iterations is 0.4821337956986508\n",
      "Gradient check score at iteration 350 is 2.5590908220106698e-15\n",
      "Cost after 360 iterations is 0.4759420732539589\n",
      "Gradient check score at iteration 360 is 8.378039105449119e-15\n",
      "Cost after 370 iterations is 0.47130087178010555\n",
      "Gradient check score at iteration 370 is 5.687566342680778e-15\n",
      "Cost after 380 iterations is 0.4677967044725646\n",
      "Gradient check score at iteration 380 is 6.3491029611853345e-15\n",
      "Cost after 390 iterations is 0.4650389087913893\n",
      "Gradient check score at iteration 390 is 8.661699405403127e-15\n",
      "Cost after 400 iterations is 0.46280015468860686\n",
      "Gradient check score at iteration 400 is 7.729637812732046e-15\n",
      "Cost after 410 iterations is 0.46094138951165453\n",
      "Gradient check score at iteration 410 is 1.077909711423631e-14\n",
      "Cost after 420 iterations is 0.45934281617159745\n",
      "Gradient check score at iteration 420 is 7.65029553150787e-15\n",
      "Cost after 430 iterations is 0.4579533151506676\n",
      "Gradient check score at iteration 430 is 1.3020348148565448e-14\n",
      "Cost after 440 iterations is 0.4566810578756728\n",
      "Gradient check score at iteration 440 is 1.1280919245621788e-14\n",
      "Cost after 450 iterations is 0.4555203981110217\n",
      "Gradient check score at iteration 450 is 1.7034758475775002e-14\n",
      "Cost after 460 iterations is 0.45447400670698224\n",
      "Gradient check score at iteration 460 is 1.7244492372965932e-14\n",
      "Cost after 470 iterations is 0.4534739230612309\n",
      "Gradient check score at iteration 470 is 8.915951860474504e-15\n",
      "Cost after 480 iterations is 0.45219418803386496\n",
      "Gradient check score at iteration 480 is 4.959883635345451e-15\n",
      "Cost after 490 iterations is 0.4507152263076593\n",
      "Gradient check score at iteration 490 is 2.1358149631408527e-14\n",
      "Cost after 500 iterations is 0.449159351359973\n",
      "Gradient check score at iteration 500 is 5.413525171421706e-15\n",
      "Cost after 510 iterations is 0.44773156993005\n",
      "Gradient check score at iteration 510 is 1.2486690570664142e-14\n",
      "Cost after 520 iterations is 0.4463362775090897\n",
      "Gradient check score at iteration 520 is 7.873413327415327e-15\n",
      "Cost after 530 iterations is 0.4447803386840345\n",
      "Gradient check score at iteration 530 is 9.031022317581961e-15\n",
      "Cost after 540 iterations is 0.443362851121564\n",
      "Gradient check score at iteration 540 is 7.037415020415614e-15\n",
      "Cost after 550 iterations is 0.4418868950404778\n",
      "Gradient check score at iteration 550 is 6.634747478884785e-15\n",
      "Cost after 560 iterations is 0.44035753265440475\n",
      "Gradient check score at iteration 560 is 9.991011537663365e-15\n",
      "Cost after 570 iterations is 0.43868136498852317\n",
      "Gradient check score at iteration 570 is 2.0621983206985257e-14\n",
      "Cost after 580 iterations is 0.4371448394872328\n",
      "Gradient check score at iteration 580 is 6.8736008577975605e-15\n",
      "Cost after 590 iterations is 0.4357474388442904\n",
      "Gradient check score at iteration 590 is 1.1522940756781455e-14\n",
      "Cost after 600 iterations is 0.4343889613072167\n",
      "Gradient check score at iteration 600 is 9.346074366341513e-15\n",
      "Cost after 610 iterations is 0.4331110290432297\n",
      "Gradient check score at iteration 610 is 5.923404374131458e-15\n",
      "Cost after 620 iterations is 0.43191125158844945\n",
      "Gradient check score at iteration 620 is 8.065476702712415e-15\n",
      "Cost after 630 iterations is 0.4306802733019493\n",
      "Gradient check score at iteration 630 is 6.771405081906446e-15\n",
      "Cost after 640 iterations is 0.42951554114067103\n",
      "Gradient check score at iteration 640 is 1.4489583043431923e-14\n",
      "Cost after 650 iterations is 0.4283715783947302\n",
      "Gradient check score at iteration 650 is 1.415127711050922e-14\n",
      "Cost after 660 iterations is 0.4272141739342812\n",
      "Gradient check score at iteration 660 is 1.2788423205994316e-14\n",
      "Cost after 670 iterations is 0.42615960116079277\n",
      "Gradient check score at iteration 670 is 3.040417336750203e-14\n",
      "Cost after 680 iterations is 0.4251440900847629\n",
      "Gradient check score at iteration 680 is 1.0391608891148822e-14\n",
      "Cost after 690 iterations is 0.42419479764675894\n",
      "Gradient check score at iteration 690 is 3.083617226521547e-14\n",
      "Cost after 700 iterations is 0.4233066436918836\n",
      "Gradient check score at iteration 700 is 1.5583891947653823e-14\n",
      "Cost after 710 iterations is 0.4224818770918341\n",
      "Gradient check score at iteration 710 is 1.6622333236493213e-14\n",
      "Cost after 720 iterations is 0.4217862780598254\n",
      "Gradient check score at iteration 720 is 1.152101037893051e-14\n",
      "Cost after 730 iterations is 0.42109565415301453\n",
      "Gradient check score at iteration 730 is 2.1421830882161243e-14\n",
      "Cost after 740 iterations is 0.4204736195849515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check score at iteration 740 is 1.7613512126665148e-14\n",
      "Cost after 750 iterations is 0.41990180205167393\n",
      "Gradient check score at iteration 750 is 1.3887722502755333e-14\n",
      "Cost after 760 iterations is 0.41932568321623626\n",
      "Gradient check score at iteration 760 is 1.9498870218663582e-14\n",
      "Cost after 770 iterations is 0.4188066258993225\n",
      "Gradient check score at iteration 770 is 2.0065752344361386e-14\n",
      "Cost after 780 iterations is 0.4182977688932318\n",
      "Gradient check score at iteration 780 is 2.4091676683602673e-14\n",
      "Cost after 790 iterations is 0.4177885549130354\n",
      "Gradient check score at iteration 790 is 1.0382404061989755e-14\n",
      "Cost after 800 iterations is 0.41730803308286796\n",
      "Gradient check score at iteration 800 is 1.8802793535513162e-14\n",
      "Cost after 810 iterations is 0.41690137551113016\n",
      "Gradient check score at iteration 810 is 9.334470598177284e-15\n",
      "Cost after 820 iterations is 0.41649354706405334\n",
      "Gradient check score at iteration 820 is 3.861011643204243e-15\n",
      "Cost after 830 iterations is 0.41604716384250745\n",
      "Gradient check score at iteration 830 is 3.292585223633745e-14\n",
      "Cost after 840 iterations is 0.41569344318975254\n",
      "Gradient check score at iteration 840 is 8.077414824974625e-15\n",
      "Cost after 850 iterations is 0.4153177426365473\n",
      "Gradient check score at iteration 850 is 2.2799437418166724e-14\n",
      "Cost after 860 iterations is 0.41497905031219934\n",
      "Gradient check score at iteration 860 is 1.602596238000693e-14\n",
      "Cost after 870 iterations is 0.41465850708694585\n",
      "Gradient check score at iteration 870 is 1.2612426908231613e-14\n",
      "Cost after 880 iterations is 0.41436559626029407\n",
      "Gradient check score at iteration 880 is 1.1447586004817354e-14\n",
      "Cost after 890 iterations is 0.41408678390600323\n",
      "Gradient check score at iteration 890 is 1.0973147983434343e-14\n",
      "Cost after 900 iterations is 0.41378312293851927\n",
      "Gradient check score at iteration 900 is 1.884392106987704e-14\n",
      "Cost after 910 iterations is 0.41349866290022125\n",
      "Gradient check score at iteration 910 is 2.1490385276394595e-14\n",
      "Cost after 920 iterations is 0.41323944220217107\n",
      "Gradient check score at iteration 920 is 2.997773743373263e-14\n",
      "Cost after 930 iterations is 0.4129858558913732\n",
      "Gradient check score at iteration 930 is 1.8330447634890268e-14\n",
      "Cost after 940 iterations is 0.41275607925816904\n",
      "Gradient check score at iteration 940 is 8.114962418678982e-15\n",
      "Cost after 950 iterations is 0.4124932511055073\n",
      "Gradient check score at iteration 950 is 6.9273143464715855e-15\n",
      "Cost after 960 iterations is 0.41222888247434925\n",
      "Gradient check score at iteration 960 is 1.3830927545976572e-14\n",
      "Cost after 970 iterations is 0.4119942508181963\n",
      "Gradient check score at iteration 970 is 2.7454796444930993e-14\n",
      "Cost after 980 iterations is 0.41177275960235904\n",
      "Gradient check score at iteration 980 is 5.629568306479421e-14\n",
      "Cost after 990 iterations is 0.41155046735532425\n",
      "Gradient check score at iteration 990 is 6.182993861582016e-14\n",
      "Cost after 1000 iterations is 0.41133591551875254\n",
      "Gradient check score at iteration 1000 is 4.8187419037261105e-14\n",
      "Cost after 1010 iterations is 0.41112542782820294\n",
      "Gradient check score at iteration 1010 is 2.847917193574097e-14\n",
      "Cost after 1020 iterations is 0.41092093194239426\n",
      "Gradient check score at iteration 1020 is 3.3284869374086164e-14\n",
      "Cost after 1030 iterations is 0.41073046517069395\n",
      "Gradient check score at iteration 1030 is 1.6752991124305674e-14\n",
      "Cost after 1040 iterations is 0.4105607911274619\n",
      "Gradient check score at iteration 1040 is 1.2735910213765772e-14\n",
      "Cost after 1050 iterations is 0.4103359571238418\n",
      "Gradient check score at iteration 1050 is 7.224074225818412e-14\n",
      "Cost after 1060 iterations is 0.41014444471606926\n",
      "Gradient check score at iteration 1060 is 5.543518849569284e-14\n",
      "Cost after 1070 iterations is 0.4099615683163056\n",
      "Gradient check score at iteration 1070 is 2.9291465110115423e-14\n",
      "Cost after 1080 iterations is 0.4099599238645397\n",
      "Gradient check score at iteration 1080 is 8.374006916435535e-16\n",
      "Cost after 1090 iterations is 0.40960372507557946\n",
      "Gradient check score at iteration 1090 is 3.105881795622232e-14\n",
      "Cost after 1100 iterations is 0.40943377795415886\n",
      "Gradient check score at iteration 1100 is 3.518565656598243e-14\n",
      "Cost after 1110 iterations is 0.40925737732321466\n",
      "Gradient check score at iteration 1110 is 1.874690756553451e-14\n",
      "Cost after 1120 iterations is 0.4090954436503338\n",
      "Gradient check score at iteration 1120 is 1.0620292681882614e-14\n",
      "Cost after 1130 iterations is 0.40896879646532547\n",
      "Gradient check score at iteration 1130 is 7.0860754834401185e-15\n",
      "Cost after 1140 iterations is 0.4088817822543502\n",
      "Gradient check score at iteration 1140 is 4.1968480869169855e-15\n",
      "Cost after 1150 iterations is 0.4087357736224883\n",
      "Gradient check score at iteration 1150 is 1.8073278615185404e-15\n",
      "Cost after 1160 iterations is 0.4085524504199254\n",
      "Gradient check score at iteration 1160 is 2.6219586922935125e-15\n",
      "Cost after 1170 iterations is 0.40835571095348977\n",
      "Gradient check score at iteration 1170 is 5.629128040809025e-15\n",
      "Cost after 1180 iterations is 0.4081795650770386\n",
      "Gradient check score at iteration 1180 is 3.565985313186284e-15\n",
      "Cost after 1190 iterations is 0.40805978814731236\n",
      "Gradient check score at iteration 1190 is 5.77869148200155e-15\n",
      "Cost after 1200 iterations is 0.4080915263558451\n",
      "Gradient check score at iteration 1200 is 3.1889831341710695e-15\n",
      "Cost after 1210 iterations is 0.4079451196510205\n",
      "Gradient check score at iteration 1210 is 1.6910893051183027e-15\n",
      "Cost after 1220 iterations is 0.40778580028755335\n",
      "Gradient check score at iteration 1220 is 1.2887211777788077e-15\n",
      "Cost after 1230 iterations is 0.407566875335851\n",
      "Gradient check score at iteration 1230 is 2.870814468623937e-15\n",
      "Cost after 1240 iterations is 0.4073849596957677\n",
      "Gradient check score at iteration 1240 is 3.3506727349000063e-15\n",
      "Cost after 1250 iterations is 0.4074046664684904\n",
      "Gradient check score at iteration 1250 is 9.992092683927255e-16\n",
      "Cost after 1260 iterations is 0.4071910723932516\n",
      "Gradient check score at iteration 1260 is 2.462070076029454e-15\n",
      "Cost after 1270 iterations is 0.40704973090596047\n",
      "Gradient check score at iteration 1270 is 3.2435331919167942e-15\n",
      "Cost after 1280 iterations is 0.40696805978570205\n",
      "Gradient check score at iteration 1280 is 1.1538498279641824e-15\n",
      "Cost after 1290 iterations is 0.40679045990635715\n",
      "Gradient check score at iteration 1290 is 2.263887103472104e-15\n",
      "Cost after 1300 iterations is 0.40652195890672843\n",
      "Gradient check score at iteration 1300 is 6.739390818613107e-14\n",
      "Cost after 1310 iterations is 0.40639223475505964\n",
      "Gradient check score at iteration 1310 is 4.732223229807828e-14\n",
      "Cost after 1320 iterations is 0.40636467212097205\n",
      "Gradient check score at iteration 1320 is 2.1771453368769055e-15\n",
      "Cost after 1330 iterations is 0.40651643829952006\n",
      "Gradient check score at iteration 1330 is 3.3480318593563277e-16\n",
      "Cost after 1340 iterations is 0.40633595411476486\n",
      "Gradient check score at iteration 1340 is 1.5804295804391285e-15\n",
      "Cost after 1350 iterations is 0.4059875736494931\n",
      "Gradient check score at iteration 1350 is 5.937058408380758e-15\n",
      "Cost after 1360 iterations is 0.4060728344822715\n",
      "Gradient check score at iteration 1360 is 1.1072026554075863e-15\n",
      "Cost after 1370 iterations is 0.40577142051380244\n",
      "Gradient check score at iteration 1370 is 1.0462872943015167e-14\n",
      "Cost after 1380 iterations is 0.40572299634970405\n",
      "Gradient check score at iteration 1380 is 1.4391615265579853e-15\n",
      "Cost after 1390 iterations is 0.40576988496055083\n",
      "Gradient check score at iteration 1390 is 9.142777892651876e-16\n",
      "Cost after 1400 iterations is 0.40552168065554595\n",
      "Gradient check score at iteration 1400 is 3.019879896469676e-15\n",
      "Cost after 1410 iterations is 0.40549753017279144\n",
      "Gradient check score at iteration 1410 is 1.625080529026876e-15\n",
      "Cost after 1420 iterations is 0.40533551445159566\n",
      "Gradient check score at iteration 1420 is 2.153653397362409e-15\n",
      "Cost after 1430 iterations is 0.4053254420029371\n",
      "Gradient check score at iteration 1430 is 1.0266471718038939e-15\n",
      "Cost after 1440 iterations is 0.40521458794155013\n",
      "Gradient check score at iteration 1440 is 1.403252931883374e-15\n",
      "Cost after 1450 iterations is 0.4049934708387259\n",
      "Gradient check score at iteration 1450 is 1.3912280081244302e-15\n",
      "Cost after 1460 iterations is 0.4050046252882386\n",
      "Gradient check score at iteration 1460 is 9.794878621311887e-16\n",
      "Cost after 1470 iterations is 0.40487560096107106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check score at iteration 1470 is 7.964952387145736e-16\n",
      "Cost after 1480 iterations is 0.4047496834043204\n",
      "Gradient check score at iteration 1480 is 1.615444006860122e-15\n",
      "Cost after 1490 iterations is 0.4046155158638338\n",
      "Gradient check score at iteration 1490 is 1.3935324816117096e-15\n",
      "Cost after 1500 iterations is 0.40455911151808943\n",
      "Gradient check score at iteration 1500 is 1.2632955712174426e-15\n",
      "Cost after 1510 iterations is 0.40433114352636795\n",
      "Gradient check score at iteration 1510 is 2.430639432710022e-15\n",
      "Cost after 1520 iterations is 0.4043244395129831\n",
      "Gradient check score at iteration 1520 is 6.754249200623544e-16\n",
      "Cost after 1530 iterations is 0.40414886951896156\n",
      "Gradient check score at iteration 1530 is 1.3291194224808625e-15\n",
      "Cost after 1540 iterations is 0.4041206412642208\n",
      "Gradient check score at iteration 1540 is 2.3008351466253925e-15\n",
      "Cost after 1550 iterations is 0.40401062492072026\n",
      "Gradient check score at iteration 1550 is 7.810801251067204e-16\n",
      "Cost after 1560 iterations is 0.40383148244353534\n",
      "Gradient check score at iteration 1560 is 8.065403874938785e-16\n",
      "Cost after 1570 iterations is 0.4038011588225405\n",
      "Gradient check score at iteration 1570 is 4.033741036209803e-15\n",
      "Cost after 1580 iterations is 0.403725505860625\n",
      "Gradient check score at iteration 1580 is 1.3349723740645364e-15\n",
      "Cost after 1590 iterations is 0.4035491889602206\n",
      "Gradient check score at iteration 1590 is 2.1153637514955673e-15\n",
      "Cost after 1600 iterations is 0.40329626110124567\n",
      "Gradient check score at iteration 1600 is 2.900718664350141e-15\n",
      "Cost after 1610 iterations is 0.4033289985362464\n",
      "Gradient check score at iteration 1610 is 1.8002052732662874e-15\n",
      "Cost after 1620 iterations is 0.40326296360379926\n",
      "Gradient check score at iteration 1620 is 1.475572381570537e-15\n",
      "Cost after 1630 iterations is 0.4029857959147127\n",
      "Gradient check score at iteration 1630 is 2.288667635351485e-15\n",
      "Cost after 1640 iterations is 0.40298554458485186\n",
      "Gradient check score at iteration 1640 is 2.582392200085043e-15\n",
      "Cost after 1650 iterations is 0.4028805649650395\n",
      "Gradient check score at iteration 1650 is 3.223141706537107e-15\n",
      "Cost after 1660 iterations is 0.40269587256094747\n",
      "Gradient check score at iteration 1660 is 2.798568640649936e-15\n",
      "Cost after 1670 iterations is 0.40263718898039774\n",
      "Gradient check score at iteration 1670 is 2.352692982800088e-15\n",
      "Cost after 1680 iterations is 0.40254938085085323\n",
      "Gradient check score at iteration 1680 is 1.3373948676994685e-15\n",
      "Cost after 1690 iterations is 0.4024057494653646\n",
      "Gradient check score at iteration 1690 is 1.883352534394854e-15\n",
      "Cost after 1700 iterations is 0.402191904414174\n",
      "Gradient check score at iteration 1700 is 1.9513403873681587e-15\n",
      "Cost after 1710 iterations is 0.40214584313713186\n",
      "Gradient check score at iteration 1710 is 1.4816794786659772e-15\n",
      "Cost after 1720 iterations is 0.40202538811987704\n",
      "Gradient check score at iteration 1720 is 2.119942271392066e-15\n",
      "Cost after 1730 iterations is 0.4019336096537594\n",
      "Gradient check score at iteration 1730 is 2.0136600381173823e-15\n",
      "Cost after 1740 iterations is 0.40176864024391484\n",
      "Gradient check score at iteration 1740 is 5.5088777213335355e-15\n",
      "Cost after 1750 iterations is 0.40168539571386785\n",
      "Gradient check score at iteration 1750 is 5.279973351211984e-15\n",
      "Cost after 1760 iterations is 0.401579888422022\n",
      "Gradient check score at iteration 1760 is 3.657427884910323e-15\n",
      "Cost after 1770 iterations is 0.40146742632314364\n",
      "Gradient check score at iteration 1770 is 1.132073664180643e-14\n",
      "Cost after 1780 iterations is 0.40138957288876426\n",
      "Gradient check score at iteration 1780 is 1.989402527105135e-15\n",
      "Cost after 1790 iterations is 0.40123839252256455\n",
      "Gradient check score at iteration 1790 is 1.6764341577058013e-14\n",
      "Cost after 1800 iterations is 0.4011470341983199\n",
      "Gradient check score at iteration 1800 is 1.8039763848236496e-14\n",
      "Cost after 1810 iterations is 0.4010463970911048\n",
      "Gradient check score at iteration 1810 is 3.8445607516236785e-14\n",
      "Cost after 1820 iterations is 0.4009598602725316\n",
      "Gradient check score at iteration 1820 is 2.0677161222067428e-14\n",
      "Cost after 1830 iterations is 0.4008781495887553\n",
      "Gradient check score at iteration 1830 is 2.601003930158186e-14\n",
      "Cost after 1840 iterations is 0.40078129960216863\n",
      "Gradient check score at iteration 1840 is 1.8586714749754954e-14\n",
      "Cost after 1850 iterations is 0.40070556964077086\n",
      "Gradient check score at iteration 1850 is 1.4450626647598477e-14\n",
      "Cost after 1860 iterations is 0.40069449815636626\n",
      "Gradient check score at iteration 1860 is 2.3848908417484453e-15\n",
      "Cost after 1870 iterations is 0.4005495140772612\n",
      "Gradient check score at iteration 1870 is 7.506757262860106e-15\n",
      "Cost after 1880 iterations is 0.4004851289014582\n",
      "Gradient check score at iteration 1880 is 8.029278440264526e-15\n",
      "Cost after 1890 iterations is 0.40043915074627573\n",
      "Gradient check score at iteration 1890 is 1.3143808642985612e-15\n",
      "Cost after 1900 iterations is 0.4003111343456806\n",
      "Gradient check score at iteration 1900 is 3.3824963529160424e-15\n",
      "Cost after 1910 iterations is 0.40027138540625107\n",
      "Gradient check score at iteration 1910 is 3.779050854872346e-15\n",
      "Cost after 1920 iterations is 0.4001699386825341\n",
      "Gradient check score at iteration 1920 is 3.6985111731792814e-15\n",
      "Cost after 1930 iterations is 0.40006334388586\n",
      "Gradient check score at iteration 1930 is 9.667386222577657e-15\n",
      "Cost after 1940 iterations is 0.3999558975426078\n",
      "Gradient check score at iteration 1940 is 1.3885891110268376e-14\n",
      "Cost after 1950 iterations is 0.40001635672700914\n",
      "Gradient check score at iteration 1950 is 1.1818981155420483e-15\n",
      "Cost after 1960 iterations is 0.39984389842854773\n",
      "Gradient check score at iteration 1960 is 4.9350639658919605e-15\n",
      "Cost after 1970 iterations is 0.3997340089633634\n",
      "Gradient check score at iteration 1970 is 1.4145656061316115e-14\n",
      "Cost after 1980 iterations is 0.3996588140422014\n",
      "Gradient check score at iteration 1980 is 1.4095487492770616e-14\n",
      "Cost after 1990 iterations is 0.3995866501157438\n",
      "Gradient check score at iteration 1990 is 5.722796384793183e-15\n"
     ]
    }
   ],
   "source": [
    "def optimize(X, Y, X_dev, Y_dev, model, print_costs=True, check_grads=False):\n",
    "    weights = init_weights(model)\n",
    "    costs = []\n",
    "    costs_dev = []\n",
    "    for i in range(model['niters']):\n",
    "        cost, grads = compute_cost_and_grads(X, Y, weights, model)\n",
    "        cost_dev = compute_cost_and_grads(X_dev, Y_dev, weights, model, backprop=False)\n",
    "        costs.append(cost)\n",
    "        costs_dev.append(cost_dev)\n",
    "        if print_costs and i % 10 == 0:\n",
    "            print('Cost after {} iterations is {}'.format(i, cost))\n",
    "        if check_grads and i % 10 == 0:\n",
    "            print('Gradient check score at iteration {} is {}'.format(i, check_gradients(X, Y, weights, grads, model)))\n",
    "        update_params(weights, grads, model['lrate'])\n",
    "    return weights, costs, costs_dev\n",
    "if False: \n",
    "    # 79.27 0.1, 300, 10 \n",
    "    # 79.55 0.1, 600, 1\n",
    "    # 80.95 0.1, 600, 2\n",
    "    # 80.11 0.14, 600, 3\n",
    "    # 81.23 0.05 2000 5\n",
    "    # 81.23 0.1 400 7\n",
    "    \n",
    "    test_size = 0.3\n",
    "    train_data, dev_data = train_test_split(train_orig, test_size=test_size)\n",
    "    train_data, norms = preprocess(train_data)\n",
    "    dev_data, _ = preprocess(dev_data, norms)\n",
    "    test_data, _ = preprocess(test_orig, norms, test=True)\n",
    "    X, Y = prepare_data(train_data)\n",
    "    X_dev, Y_dev = prepare_data(dev_data)\n",
    "    X_test = test_data.as_matrix().T\n",
    "    \n",
    "    model = get_model_config(**{\n",
    "        'activation_fn': 'relu',\n",
    "        'cost_fn': 'log_cost',\n",
    "        'layer_dims': [X.shape[0], 10, 10, Y.shape[0]],\n",
    "        'lrate': 0.05,\n",
    "        'niters': 2000,\n",
    "        'test_size': test_size,\n",
    "        'reg_fn': 'l2',\n",
    "        'reg_factor': 0.001\n",
    "    })\n",
    "    weights, costs, costs_dev = optimize(X, Y, X_dev, Y_dev, model, check_grads=True)\n",
    "if True:    \n",
    "    test_size = 0.3\n",
    "    train_data, dev_data = train_test_split(train_orig, test_size=test_size)\n",
    "    train_data, computed = preprocess_advanced(train_data)\n",
    "    dev_data, _ = preprocess_advanced(dev_data, computed)\n",
    "    test_data, _ = preprocess_advanced(test_orig, computed)\n",
    "    X, Y = prepare_data(train_data)\n",
    "    X_dev, Y_dev = prepare_data(dev_data)\n",
    "    X_test = test_data.as_matrix().T\n",
    "    \n",
    "    model = get_model_config(**{\n",
    "        'activation_fn': 'relu',\n",
    "        'cost_fn': 'log_cost',\n",
    "        'layer_dims': [X.shape[0], 7, 3, Y.shape[0]],\n",
    "        'lrate': 0.05,\n",
    "        'niters': 2000,\n",
    "        'test_size': test_size,\n",
    "        'reg_fn': 'l2',\n",
    "        'reg_factor': 0.001\n",
    "    })\n",
    "    weights, costs, costs_dev = optimize(X, Y, X_dev, Y_dev, model, check_grads=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "6d2304b2-7aff-4197-8fbb-adf60de9d5b2",
    "_uuid": "a2feed6c8238a476e8f0f559098d0f31aa2e3e8e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pylab.rcParams['figure.figsize'] = (20, 6)\n",
    "def learning_curve(costs, niters):\n",
    "    x = np.arange(niters)\n",
    "    y = costs\n",
    "    plt.plot(x,y, color='blue')\n",
    "    y = costs_dev\n",
    "    plt.plot(x,y, color='green')\n",
    "    plt.show()\n",
    "if True:\n",
    "    learning_curve(costs, model['niters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "63d4169c-629d-4a55-97c4-8fad3c091c93",
    "_uuid": "b8d8c954380f3d2233fe02f54131c1a2a54ff136"
   },
   "outputs": [],
   "source": [
    "def predict(X, weights, model):\n",
    "    Afinal = X\n",
    "#     print(X.shape, weights[0].W.shape, weights[0].B.shape, weights[1].W.shape)\n",
    "    for i, weight in enumerate(weights):\n",
    "#         print(weight.W.shape, weight.B.shape, Afinal.shape)\n",
    "        Z = np.dot(weight.W, Afinal) + weight.B\n",
    "        curr_act = get_activation_fn(model['activation_fn'] if i < len(weights)-1 else 'sigmoid')\n",
    "        Afinal = curr_act(Z)\n",
    "    return np.where(Afinal<=0.5, 0, 1)\n",
    "if True:\n",
    "    predictions = predict(X, weights, model)\n",
    "    predictions_dev = predict(X_dev, weights, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "82c919c6-00b7-47c2-8161-0e4bddcd94b1",
    "_uuid": "bdf7060b2d01aa999ac6d17b0096c8ec00f0d8cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 82.6645264847512%\n",
      "Dev Accuracy: 83.2089552238806%\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(Y, predictions):\n",
    "    return (1 - (np.abs(Y - predictions).sum() / Y.shape[1])) * 100\n",
    "if True:\n",
    "    accuracy = compute_accuracy(Y, predictions)\n",
    "    accuracy_dev = compute_accuracy(Y_dev, predictions_dev)\n",
    "    print('Training Accuracy: {}%'.format(accuracy))\n",
    "    print('Dev Accuracy: {}%'.format(accuracy_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "c6a2d90d-4b2f-46d4-8cd4-5bf54441f010",
    "_uuid": "91130ea9bd620b5fc1a229963e2ab24135268e84"
   },
   "outputs": [],
   "source": [
    "# Grid Search (Random) - Hyperparameter Tuning\n",
    "def get_random_list(items, size, weights=None):\n",
    "    if weights is None:\n",
    "        weights = [1 for _ in range(len(items))]\n",
    "    weights = np.round((np.array(weights) * size) / np.sum(weights))\n",
    "    weights[0] = size - np.sum(weights[1:])\n",
    "    random_list = []\n",
    "    for i in range(len(items)):\n",
    "        random_list.extend([items[i]]*int(weights[i]))\n",
    "    np.random.shuffle(random_list)\n",
    "    return random_list\n",
    "    \n",
    "def get_grid(size, nx=7, ny=1):\n",
    "    activation_fns = get_random_list(['relu', 'tanh', 'sigmoid'], size, [4,2,1])\n",
    "    cost_fns = get_random_list(['log_cost', 'mse'], size, [7, 3])\n",
    "    num_layers = np.random.randint(1, 5, size)\n",
    "    num_neurons = np.random.randint(2, 10, size)\n",
    "    layer_dims = [[nx, *[num_neurons[i] for j in range(num_layers[i])], ny] for i in range(size)]\n",
    "    lrates = np.power(10, np.random.uniform(-4, 1, size))\n",
    "    reg_fns = get_random_list(['l2', None], size, [8, 2])\n",
    "    reg_factors = np.power(10, np.random.uniform(-5, 1, size))\n",
    "    models = []\n",
    "    for i in range(size):\n",
    "        models.append(get_model_config(**{\n",
    "        'activation_fn': activation_fns[i],\n",
    "        'cost_fn': cost_fns[i],\n",
    "        'layer_dims': layer_dims[i],\n",
    "        'lrate': lrates[i],\n",
    "        'niters': 2000,\n",
    "        'test_size': 0.1,\n",
    "        'reg_fn': reg_fns[i],\n",
    "        'reg_factor': reg_factors[i]\n",
    "        }))\n",
    "    return models\n",
    "\n",
    "def get_grid_(size, nx=7, ny=1):\n",
    "    activation_fns = get_random_list(['relu', 'tanh', 'sigmoid'], size, [4,2,1])\n",
    "    cost_fns = get_random_list(['log_cost'], size)\n",
    "    num_layers = np.random.randint(1, 5, size)\n",
    "    num_neurons = np.random.randint(2, 10, size)\n",
    "    layer_dims = [[nx, *[num_neurons[i] for j in range(num_layers[i])], ny] for i in range(size)]\n",
    "    lrates = np.power(10, np.random.uniform(-4, 1, size))\n",
    "    reg_fns = get_random_list(['l2', None], size, [8, 2])\n",
    "    reg_factors = np.power(10, np.random.uniform(-5, 1, size))\n",
    "    models = []\n",
    "    for i in range(size):\n",
    "        models.append(get_model_config(**{\n",
    "        'activation_fn': 'relu',\n",
    "        'cost_fn': 'log_cost',\n",
    "        'layer_dims': layer_dims[i],\n",
    "        'lrate': lrates[i],\n",
    "        'niters': 2000,\n",
    "        'test_size': 0.25,\n",
    "        'reg_fn': 'l2',\n",
    "        'reg_factor': reg_factors[i]\n",
    "        }))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "a4791ba4-1e86-4c40-9f64-042b93c31253",
    "_uuid": "cdb56371d607f2f1241ce2c56e32b50b33f457ee",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model 1 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 8, 8, 8, 8, 1]\n",
      "lrate 1.594833479291636\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.04890633598442833\n",
      "test_size 0.1\n",
      "Model accuracy: Train=86.26716604244695%, Dev=81.11111111111111%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=81.11111111111111%\n",
      "Evaluating model 2 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 2, 2, 2, 2, 1]\n",
      "lrate 4.020073901526279\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.09249422769696948\n",
      "test_size 0.1\n",
      "Model accuracy: Train=81.27340823970037%, Dev=84.44444444444444%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=84.44444444444444%\n",
      "Evaluating model 3 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn mse\n",
      "layer_dims [7, 9, 1]\n",
      "lrate 0.00046117823237266605\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.00885322223018704\n",
      "test_size 0.1\n",
      "Model accuracy: Train=60.17478152309613%, Dev=65.55555555555556%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=84.44444444444444%\n",
      "Evaluating model 4 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 4, 4, 4, 4, 1]\n",
      "lrate 5.4056445254263705\n",
      "niters 2000\n",
      "reg_fn None\n",
      "reg_factor 5.386019283217205\n",
      "test_size 0.1\n",
      "Model accuracy: Train=61.048689138576776%, Dev=66.66666666666667%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=84.44444444444444%\n",
      "Evaluating model 5 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn mse\n",
      "layer_dims [7, 2, 2, 2, 1]\n",
      "lrate 0.14720040352332087\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 1.264456273184114\n",
      "test_size 0.1\n",
      "Model accuracy: Train=61.048689138576776%, Dev=66.66666666666667%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=84.44444444444444%\n",
      "Evaluating model 6 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 9, 9, 9, 1]\n",
      "lrate 0.014970426179196958\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 3.520911859770451\n",
      "test_size 0.1\n",
      "Model accuracy: Train=80.14981273408239%, Dev=84.44444444444444%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=84.44444444444444%\n",
      "Evaluating model 7 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 6, 6, 1]\n",
      "lrate 0.0044839086886007355\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 1.172511825137733\n",
      "test_size 0.1\n",
      "Model accuracy: Train=79.02621722846442%, Dev=84.44444444444444%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=84.44444444444444%\n",
      "Evaluating model 8 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 3, 3, 3, 3, 1]\n",
      "lrate 0.0001467617167463637\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 3.947685058806523\n",
      "test_size 0.1\n",
      "Model accuracy: Train=63.79525593008739%, Dev=72.22222222222221%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=84.44444444444444%\n",
      "Evaluating model 9 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 6, 6, 6, 1]\n",
      "lrate 2.353660208577631\n",
      "niters 2000\n",
      "reg_fn None\n",
      "reg_factor 4.048861992608207\n",
      "test_size 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rbiswas/.virtualenvs/kaggle3/lib/python3.6/site-packages/ipykernel_launcher.py:48: RuntimeWarning: divide by zero encountered in log\n",
      "/home/rbiswas/.virtualenvs/kaggle3/lib/python3.6/site-packages/ipykernel_launcher.py:53: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/rbiswas/.virtualenvs/kaggle3/lib/python3.6/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in less_equal\n",
      "  app.launch_new_instance()\n",
      "/home/rbiswas/.virtualenvs/kaggle3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in less_equal\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: Train=38.95131086142322%, Dev=33.333333333333336%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=84.44444444444444%\n",
      "Evaluating model 10 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 5, 5, 1]\n",
      "lrate 0.012474221064492206\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 2.2679159451266895e-05\n",
      "test_size 0.1\n",
      "Model accuracy: Train=69.6629213483146%, Dev=78.88888888888889%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=84.44444444444444%\n",
      "Evaluating model 11 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 3, 3, 1]\n",
      "lrate 0.030845271019837788\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 8.755210806792972e-05\n",
      "test_size 0.1\n",
      "Model accuracy: Train=79.7752808988764%, Dev=86.66666666666667%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 12 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 9, 9, 9, 1]\n",
      "lrate 0.557456259031156\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 2.194575121766235\n",
      "test_size 0.1\n",
      "Model accuracy: Train=83.14606741573034%, Dev=86.66666666666667%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 13 of 80\n",
      "Model info\n",
      "activation_fn sigmoid\n",
      "cost_fn mse\n",
      "layer_dims [7, 2, 1]\n",
      "lrate 0.02685381307491657\n",
      "niters 2000\n",
      "reg_fn None\n",
      "reg_factor 0.00025546426744106783\n",
      "test_size 0.1\n",
      "Model accuracy: Train=61.048689138576776%, Dev=66.66666666666667%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 14 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 7, 1]\n",
      "lrate 3.6079787331998316\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.016207426461025572\n",
      "test_size 0.1\n",
      "Model accuracy: Train=85.0187265917603%, Dev=85.55555555555556%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 15 of 80\n",
      "Model info\n",
      "activation_fn sigmoid\n",
      "cost_fn mse\n",
      "layer_dims [7, 4, 4, 1]\n",
      "lrate 8.57135515158794\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.2569392357180965\n",
      "test_size 0.1\n",
      "Model accuracy: Train=38.95131086142322%, Dev=33.333333333333336%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 16 of 80\n",
      "Model info\n",
      "activation_fn sigmoid\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 7, 7, 1]\n",
      "lrate 0.00010141386956356402\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 1.9944866763049275e-05\n",
      "test_size 0.1\n",
      "Model accuracy: Train=61.048689138576776%, Dev=66.66666666666667%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 17 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn mse\n",
      "layer_dims [7, 2, 2, 2, 1]\n",
      "lrate 0.15381793000503624\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 7.335733620436968e-05\n",
      "test_size 0.1\n",
      "Model accuracy: Train=61.048689138576776%, Dev=66.66666666666667%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 18 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 6, 6, 1]\n",
      "lrate 1.5139253933420227\n",
      "niters 2000\n",
      "reg_fn None\n",
      "reg_factor 0.5431518184261949\n",
      "test_size 0.1\n",
      "Model accuracy: Train=82.27215980024968%, Dev=84.44444444444444%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 19 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn mse\n",
      "layer_dims [7, 3, 3, 3, 3, 1]\n",
      "lrate 0.0005405478938102554\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.00040240352330840975\n",
      "test_size 0.1\n",
      "Model accuracy: Train=61.048689138576776%, Dev=66.66666666666667%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 20 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 4, 4, 4, 4, 1]\n",
      "lrate 1.0338131161472417\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 4.149875365752091\n",
      "test_size 0.1\n",
      "Model accuracy: Train=80.64918851435705%, Dev=82.22222222222221%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 21 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn mse\n",
      "layer_dims [7, 6, 6, 6, 1]\n",
      "lrate 0.09348833085702553\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 1.9357182742763301\n",
      "test_size 0.2\n",
      "Model accuracy: Train=61.65730337078652%, Dev=61.452513966480446%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 22 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 3, 3, 3, 3, 1]\n",
      "lrate 0.0005088477125384387\n",
      "niters 2000\n",
      "reg_fn None\n",
      "reg_factor 8.26738281294588\n",
      "test_size 0.2\n",
      "Model accuracy: Train=64.18539325842696%, Dev=66.4804469273743%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 23 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 2, 2, 2, 1]\n",
      "lrate 2.756715272545156\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.2100479990954636\n",
      "test_size 0.2\n",
      "Model accuracy: Train=81.17977528089888%, Dev=81.56424581005587%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 24 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 2, 2, 2, 2, 1]\n",
      "lrate 0.0275581098539503\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 4.286436169159279\n",
      "test_size 0.2\n",
      "Model accuracy: Train=80.19662921348313%, Dev=81.56424581005587%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 25 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn mse\n",
      "layer_dims [7, 3, 3, 3, 3, 1]\n",
      "lrate 0.00013477678555839736\n",
      "niters 2000\n",
      "reg_fn None\n",
      "reg_factor 0.0035151219220287515\n",
      "test_size 0.2\n",
      "Model accuracy: Train=61.65730337078652%, Dev=61.452513966480446%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 26 of 80\n",
      "Model info\n",
      "activation_fn sigmoid\n",
      "cost_fn mse\n",
      "layer_dims [7, 4, 4, 1]\n",
      "lrate 0.0003666932613512331\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 6.108931941778612e-05\n",
      "test_size 0.2\n",
      "Model accuracy: Train=61.65730337078652%, Dev=61.452513966480446%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 27 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 7, 7, 7, 1]\n",
      "lrate 3.422353203500641\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.18499185281803576\n",
      "test_size 0.2\n",
      "Model accuracy: Train=80.6179775280899%, Dev=81.00558659217877%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 28 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 5, 5, 1]\n",
      "lrate 1.5352041548531068\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.0026874692016683123\n",
      "test_size 0.2\n",
      "Model accuracy: Train=84.5505617977528%, Dev=82.12290502793296%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 29 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 8, 1]\n",
      "lrate 0.013530466693286197\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.000586064611160377\n",
      "test_size 0.2\n",
      "Model accuracy: Train=79.49438202247191%, Dev=81.56424581005587%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 30 of 80\n",
      "Model info\n",
      "activation_fn sigmoid\n",
      "cost_fn mse\n",
      "layer_dims [7, 8, 8, 1]\n",
      "lrate 0.0002082107033002507\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.9489349251988373\n",
      "test_size 0.2\n",
      "Model accuracy: Train=61.65730337078652%, Dev=61.452513966480446%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 31 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 2, 1]\n",
      "lrate 0.0023590440206090776\n",
      "niters 2000\n",
      "reg_fn None\n",
      "reg_factor 0.005656399202506748\n",
      "test_size 0.2\n",
      "Model accuracy: Train=64.74719101123596%, Dev=65.36312849162012%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 32 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 4, 4, 4, 4, 1]\n",
      "lrate 0.017024823559924882\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 6.050349568942672\n",
      "test_size 0.2\n",
      "Model accuracy: Train=61.65730337078652%, Dev=61.452513966480446%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 33 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 8, 8, 8, 1]\n",
      "lrate 0.17017511154292567\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.0028510614968755314\n",
      "test_size 0.2\n",
      "Model accuracy: Train=83.00561797752809%, Dev=83.24022346368716%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 34 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 6, 6, 6, 6, 1]\n",
      "lrate 0.0030730559143264274\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 2.6047121790104175e-05\n",
      "test_size 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: Train=78.93258426966293%, Dev=81.00558659217877%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 35 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn mse\n",
      "layer_dims [7, 5, 1]\n",
      "lrate 0.001973112842934473\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.6059313587407258\n",
      "test_size 0.2\n",
      "Model accuracy: Train=61.37640449438202%, Dev=60.89385474860336%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 36 of 80\n",
      "Model info\n",
      "activation_fn sigmoid\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 4, 4, 1]\n",
      "lrate 4.678069091615476\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.038286336652165326\n",
      "test_size 0.2\n",
      "Model accuracy: Train=83.98876404494382%, Dev=82.68156424581005%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 37 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 8, 8, 1]\n",
      "lrate 0.007883846067676484\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.04965425525600484\n",
      "test_size 0.2\n",
      "Model accuracy: Train=79.07303370786516%, Dev=81.56424581005587%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 38 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn mse\n",
      "layer_dims [7, 8, 8, 8, 1]\n",
      "lrate 0.08015039511343312\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.7305933545829264\n",
      "test_size 0.2\n",
      "Model accuracy: Train=38.34269662921348%, Dev=38.547486033519554%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 39 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 8, 8, 8, 8, 1]\n",
      "lrate 0.0015902841543152474\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 1.9017920376873382e-05\n",
      "test_size 0.2\n",
      "Model accuracy: Train=79.91573033707866%, Dev=81.00558659217877%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 40 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 8, 8, 1]\n",
      "lrate 0.03572389683971155\n",
      "niters 2000\n",
      "reg_fn None\n",
      "reg_factor 9.524234709296526e-05\n",
      "test_size 0.2\n",
      "Model accuracy: Train=82.7247191011236%, Dev=81.56424581005587%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 41 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 8, 8, 8, 1]\n",
      "lrate 0.0009394523001724593\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.015461056584896937\n",
      "test_size 0.3\n",
      "Model accuracy: Train=62.2792937399679%, Dev=65.29850746268657%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 42 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn mse\n",
      "layer_dims [7, 5, 5, 5, 5, 1]\n",
      "lrate 0.272784373953453\n",
      "niters 2000\n",
      "reg_fn None\n",
      "reg_factor 0.00011971849364186294\n",
      "test_size 0.3\n",
      "Model accuracy: Train=39.1653290529695%, Dev=36.56716417910447%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 43 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn mse\n",
      "layer_dims [7, 9, 9, 9, 1]\n",
      "lrate 0.004443935282978399\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.37777362710572526\n",
      "test_size 0.3\n",
      "Model accuracy: Train=60.8346709470305%, Dev=63.43283582089552%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 44 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 5, 5, 5, 1]\n",
      "lrate 0.2349373872776167\n",
      "niters 2000\n",
      "reg_fn None\n",
      "reg_factor 2.163369184082486\n",
      "test_size 0.3\n",
      "Model accuracy: Train=82.98555377207062%, Dev=80.59701492537313%\n",
      "Best train accuracy=86.26716604244695% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 45 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 9, 9, 1]\n",
      "lrate 3.3958078693170863\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.000540495119376955\n",
      "test_size 0.3\n",
      "Model accuracy: Train=87.47993579454254%, Dev=80.22388059701493%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 46 of 80\n",
      "Model info\n",
      "activation_fn sigmoid\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 7, 7, 1]\n",
      "lrate 0.0001670866903323226\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 3.5664146139859882\n",
      "test_size 0.3\n",
      "Model accuracy: Train=60.8346709470305%, Dev=63.43283582089552%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 47 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 9, 9, 9, 9, 1]\n",
      "lrate 0.09627892270619887\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.009322460681388827\n",
      "test_size 0.3\n",
      "Model accuracy: Train=83.62760834670947%, Dev=82.08955223880598%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 48 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 7, 7, 7, 7, 1]\n",
      "lrate 0.009525125837034043\n",
      "niters 2000\n",
      "reg_fn None\n",
      "reg_factor 7.761858702334194e-05\n",
      "test_size 0.3\n",
      "Model accuracy: Train=80.73836276083468%, Dev=80.59701492537313%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 49 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn mse\n",
      "layer_dims [7, 8, 8, 1]\n",
      "lrate 5.212787470405692\n",
      "niters 2000\n",
      "reg_fn None\n",
      "reg_factor 0.0008468965358032659\n",
      "test_size 0.3\n",
      "Model accuracy: Train=60.8346709470305%, Dev=63.43283582089552%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 50 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 9, 1]\n",
      "lrate 0.004074902280976759\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 2.3388064709683064e-05\n",
      "test_size 0.3\n",
      "Model accuracy: Train=79.45425361155698%, Dev=77.23880597014924%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 51 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 4, 1]\n",
      "lrate 0.013920583931689625\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.00010833339738898175\n",
      "test_size 0.3\n",
      "Model accuracy: Train=80.73836276083468%, Dev=79.8507462686567%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 52 of 80\n",
      "Model info\n",
      "activation_fn sigmoid\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 6, 6, 1]\n",
      "lrate 4.077104311253964\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.00011743989764947554\n",
      "test_size 0.3\n",
      "Model accuracy: Train=83.62760834670947%, Dev=77.98507462686567%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 53 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 8, 8, 1]\n",
      "lrate 0.002468349654441166\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.6804524547627495\n",
      "test_size 0.3\n",
      "Model accuracy: Train=75.76243980738363%, Dev=76.11940298507463%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 54 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 7, 1]\n",
      "lrate 0.6720779384022143\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 1.0267223905422556\n",
      "test_size 0.3\n",
      "Model accuracy: Train=83.14606741573034%, Dev=82.08955223880598%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 55 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 6, 1]\n",
      "lrate 0.0022826526334131666\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 4.336407723921355e-05\n",
      "test_size 0.3\n",
      "Model accuracy: Train=78.65168539325843%, Dev=81.34328358208955%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 56 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 4, 4, 4, 1]\n",
      "lrate 0.0001302737187225738\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 6.235248185173817e-05\n",
      "test_size 0.3\n",
      "Model accuracy: Train=43.17817014446228%, Dev=44.029850746268664%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 57 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn mse\n",
      "layer_dims [7, 3, 3, 1]\n",
      "lrate 0.07129715712713196\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 2.327671319235943e-05\n",
      "test_size 0.3\n",
      "Model accuracy: Train=60.8346709470305%, Dev=63.43283582089552%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 58 of 80\n",
      "Model info\n",
      "activation_fn sigmoid\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 6, 6, 6, 1]\n",
      "lrate 0.00010548037197026602\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 6.384224513637478e-05\n",
      "test_size 0.3\n",
      "Model accuracy: Train=60.8346709470305%, Dev=63.43283582089552%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 59 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn mse\n",
      "layer_dims [7, 6, 6, 6, 1]\n",
      "lrate 3.3827267648781385\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.0818809655926644\n",
      "test_size 0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: Train=60.8346709470305%, Dev=63.43283582089552%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 60 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn mse\n",
      "layer_dims [7, 7, 7, 1]\n",
      "lrate 0.0006402058041932246\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 9.98102868913216\n",
      "test_size 0.3\n",
      "Model accuracy: Train=62.2792937399679%, Dev=64.55223880597015%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 61 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 5, 5, 5, 1]\n",
      "lrate 0.983289672868123\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.0050160698587911655\n",
      "test_size 0.4\n",
      "Model accuracy: Train=86.32958801498127%, Dev=77.87114845938376%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 62 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 2, 2, 1]\n",
      "lrate 0.00028254686847008475\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 6.141743777424628\n",
      "test_size 0.4\n",
      "Model accuracy: Train=62.546816479400746%, Dev=60.22408963585435%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 63 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn mse\n",
      "layer_dims [7, 7, 7, 1]\n",
      "lrate 2.4100311262125778\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.031473237648850357\n",
      "test_size 0.4\n",
      "Model accuracy: Train=65.73033707865167%, Dev=61.34453781512605%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 64 of 80\n",
      "Model info\n",
      "activation_fn sigmoid\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 8, 8, 8, 1]\n",
      "lrate 0.0012813709346296287\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 3.267955610195561\n",
      "test_size 0.4\n",
      "Model accuracy: Train=62.546816479400746%, Dev=60.22408963585435%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 65 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn mse\n",
      "layer_dims [7, 9, 9, 9, 1]\n",
      "lrate 0.012646964479626151\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.5021169152059929\n",
      "test_size 0.4\n",
      "Model accuracy: Train=62.546816479400746%, Dev=60.22408963585435%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 66 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 9, 9, 1]\n",
      "lrate 7.493180287303805\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.0010145096466503065\n",
      "test_size 0.4\n",
      "Model accuracy: Train=81.08614232209737%, Dev=75.35014005602241%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 67 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn mse\n",
      "layer_dims [7, 6, 6, 1]\n",
      "lrate 0.012245843998051596\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 5.55578817594431e-05\n",
      "test_size 0.4\n",
      "Model accuracy: Train=62.546816479400746%, Dev=60.22408963585435%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 68 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 2, 2, 2, 2, 1]\n",
      "lrate 5.122144795355572\n",
      "niters 2000\n",
      "reg_fn None\n",
      "reg_factor 0.38047173140907947\n",
      "test_size 0.4\n",
      "Model accuracy: Train=62.546816479400746%, Dev=60.22408963585435%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 69 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 4, 4, 4, 1]\n",
      "lrate 0.3135997873066088\n",
      "niters 2000\n",
      "reg_fn None\n",
      "reg_factor 0.001643669430788889\n",
      "test_size 0.4\n",
      "Model accuracy: Train=84.45692883895131%, Dev=79.55182072829132%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 70 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 4, 4, 1]\n",
      "lrate 6.956065162377972\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.0023989690954896053\n",
      "test_size 0.4\n",
      "Model accuracy: Train=62.546816479400746%, Dev=60.22408963585435%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 71 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 7, 7, 7, 1]\n",
      "lrate 0.00020635266743064182\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.0001597075014971071\n",
      "test_size 0.4\n",
      "Model accuracy: Train=70.0374531835206%, Dev=68.62745098039215%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 72 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 4, 4, 4, 4, 1]\n",
      "lrate 0.0028681872354097953\n",
      "niters 2000\n",
      "reg_fn None\n",
      "reg_factor 2.918590700553858\n",
      "test_size 0.4\n",
      "Model accuracy: Train=62.546816479400746%, Dev=60.22408963585435%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 73 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 8, 1]\n",
      "lrate 0.0047441929644932304\n",
      "niters 2000\n",
      "reg_fn None\n",
      "reg_factor 0.12703762841431468\n",
      "test_size 0.4\n",
      "Model accuracy: Train=78.08988764044943%, Dev=78.71148459383754%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 74 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 5, 1]\n",
      "lrate 0.0021430704464995265\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.021395985484294888\n",
      "test_size 0.4\n",
      "Model accuracy: Train=72.84644194756554%, Dev=71.42857142857143%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 75 of 80\n",
      "Model info\n",
      "activation_fn sigmoid\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 9, 1]\n",
      "lrate 0.4945661520336541\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.008417786394508079\n",
      "test_size 0.4\n",
      "Model accuracy: Train=83.89513108614233%, Dev=79.55182072829132%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 76 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 3, 1]\n",
      "lrate 0.00015358071355444633\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.007749773005740741\n",
      "test_size 0.4\n",
      "Model accuracy: Train=56.17977528089888%, Dev=52.38095238095239%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 77 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn mse\n",
      "layer_dims [7, 9, 9, 9, 1]\n",
      "lrate 0.008631058433793965\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.00013714994983610933\n",
      "test_size 0.4\n",
      "Model accuracy: Train=63.670411985018724%, Dev=61.06442577030813%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 78 of 80\n",
      "Model info\n",
      "activation_fn relu\n",
      "cost_fn mse\n",
      "layer_dims [7, 2, 2, 2, 1]\n",
      "lrate 1.2485950853679226\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.01341628574882259\n",
      "test_size 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rbiswas/.virtualenvs/kaggle3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/rbiswas/.virtualenvs/kaggle3/lib/python3.6/site-packages/ipykernel_launcher.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  import sys\n",
      "/home/rbiswas/.virtualenvs/kaggle3/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  \n",
      "/home/rbiswas/.virtualenvs/kaggle3/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in square\n",
      "  \n",
      "/home/rbiswas/.virtualenvs/kaggle3/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: Train=37.453183520599254%, Dev=39.77591036414566%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 79 of 80\n",
      "Model info\n",
      "activation_fn tanh\n",
      "cost_fn mse\n",
      "layer_dims [7, 5, 5, 1]\n",
      "lrate 0.00014998441534726646\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 0.0413443219648243\n",
      "test_size 0.4\n",
      "Model accuracy: Train=63.48314606741573%, Dev=60.504201680672274%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n",
      "Evaluating model 80 of 80\n",
      "Model info\n",
      "activation_fn sigmoid\n",
      "cost_fn log_cost\n",
      "layer_dims [7, 8, 1]\n",
      "lrate 0.5765439979012045\n",
      "niters 2000\n",
      "reg_fn l2\n",
      "reg_factor 7.138989470170806e-05\n",
      "test_size 0.4\n",
      "Model accuracy: Train=83.70786516853931%, Dev=78.71148459383754%\n",
      "Best train accuracy=87.47993579454254% and best dev accuracy=86.66666666666667%\n"
     ]
    }
   ],
   "source": [
    "def grid_search(samples=20, test_sizes = [0.1, 0.25, 0.4]):\n",
    "    data_dicts = []\n",
    "    best_training_accuracy = 0\n",
    "    best_dev_accuracy = 0\n",
    "    for i, test_size in enumerate(test_sizes):\n",
    "        train_data, dev_data = train_test_split(train_orig, test_size=test_size)\n",
    "        train_data, norms = preprocess(train_data)\n",
    "        dev_data, _ = preprocess(dev_data, norms)\n",
    "        X, Y = prepare_data(train_data)\n",
    "        X_dev, Y_dev = prepare_data(dev_data)\n",
    "\n",
    "        models = get_grid(samples, nx=X.shape[0], ny=1)\n",
    "        for model in models:\n",
    "            model['test_size'] = test_size\n",
    "        \n",
    "        for j,model in enumerate(models):\n",
    "            print('Evaluating model {} of {}'.format(j+1 + (i*samples), samples*len(test_sizes)))\n",
    "            print('Model info')\n",
    "            for k in model:\n",
    "                print(k, model[k])\n",
    "            weights, costs, costs_dev = optimize(X, Y, X_dev, Y_dev, model, check_grads=False, print_costs=False)\n",
    "            \n",
    "            predictions = predict(X, weights, model)\n",
    "            predictions_dev = predict(X_dev, weights, model)\n",
    "            \n",
    "            accuracy = compute_accuracy(Y, predictions)\n",
    "            best_training_accuracy = max(best_training_accuracy, accuracy)\n",
    "            accuracy_dev = compute_accuracy(Y_dev, predictions_dev)\n",
    "            best_dev_accuracy = max(best_dev_accuracy, accuracy_dev)\n",
    "            print('Model accuracy: Train={}%, Dev={}%'.format(accuracy, accuracy_dev))\n",
    "            print('Best train accuracy={}% and best dev accuracy={}%'.format(best_training_accuracy, best_dev_accuracy))\n",
    "        \n",
    "            data_dicts.append(dict({\n",
    "                'test_size': test_size,\n",
    "                'train_data': train_data,\n",
    "                'dev_data': dev_data,\n",
    "                'norms': norms,\n",
    "                'X': X,\n",
    "                'Y': Y,\n",
    "                'X_dev': X_dev,\n",
    "                'Y_dev': Y_dev,\n",
    "                'weights': weights,\n",
    "                'costs': costs,\n",
    "                'costs_dev': costs_dev,\n",
    "                'predictions': predictions,\n",
    "                'predictions_dev': predictions_dev,\n",
    "                'accuracy': accuracy,\n",
    "                'accuracy_dev': accuracy_dev,\n",
    "                'num_layers': len(model['layer_dims'])-2,\n",
    "                'num_neurons': model['layer_dims'][1] \n",
    "            }, **model))\n",
    "            \n",
    "    return data_dicts\n",
    "\n",
    "if True:\n",
    "    grid_data = grid_search(samples=20, test_sizes=[0.1, 0.2, 0.3, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "c4082079-03ce-4bb8-9094-fe1d83c45bd5",
    "_uuid": "9daa7316a2f2bffdc8b710bec3c9791ee0391a68"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    pd.DataFrame(grid_data).to_csv('./saved/grid_data_4test_sizes_40samples.csv', index=False)\n",
    "    print('DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    file_ = './saved/grid_data_4test_sizes_40samples.pickled'\n",
    "    with open(file_, 'wb') as fp:\n",
    "        pickle.dump(grid_data, fp)\n",
    "    print('DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "88cfc628-f86b-4238-a368-58fc975966b0",
    "_uuid": "d6ba19b3af52782fa05d0095466ad5297d1b6fb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     accuracy  accuracy_dev activation_fn   cost_fn     lrate reg_fn  \\\n",
      "11  83.146067     86.666667          tanh  log_cost  0.557456     l2   \n",
      "10  79.775281     86.666667          relu  log_cost  0.030845     l2   \n",
      "13  85.018727     85.555556          tanh  log_cost  3.607979     l2   \n",
      "1   81.273408     84.444444          tanh  log_cost  4.020074     l2   \n",
      "17  82.272160     84.444444          relu  log_cost  1.513925   None   \n",
      "5   80.149813     84.444444          tanh  log_cost  0.014970     l2   \n",
      "6   79.026217     84.444444          relu  log_cost  0.004484     l2   \n",
      "32  83.005618     83.240223          relu  log_cost  0.170175     l2   \n",
      "35  83.988764     82.681564       sigmoid  log_cost  4.678069     l2   \n",
      "19  80.649189     82.222222          relu  log_cost  1.033813     l2   \n",
      "27  84.550562     82.122905          tanh  log_cost  1.535204     l2   \n",
      "46  83.627608     82.089552          relu  log_cost  0.096279     l2   \n",
      "53  83.146067     82.089552          relu  log_cost  0.672078     l2   \n",
      "23  80.196629     81.564246          relu  log_cost  0.027558     l2   \n",
      "22  81.179775     81.564246          tanh  log_cost  2.756715     l2   \n",
      "39  82.724719     81.564246          tanh  log_cost  0.035724   None   \n",
      "28  79.494382     81.564246          relu  log_cost  0.013530     l2   \n",
      "36  79.073034     81.564246          relu  log_cost  0.007884     l2   \n",
      "54  78.651685     81.343284          tanh  log_cost  0.002283     l2   \n",
      "0   86.267166     81.111111          tanh  log_cost  1.594833     l2   \n",
      "38  79.915730     81.005587          tanh  log_cost  0.001590     l2   \n",
      "26  80.617978     81.005587          relu  log_cost  3.422353     l2   \n",
      "33  78.932584     81.005587          relu  log_cost  0.003073     l2   \n",
      "43  82.985554     80.597015          relu  log_cost  0.234937   None   \n",
      "47  80.738363     80.597015          relu  log_cost  0.009525   None   \n",
      "44  87.479936     80.223881          tanh  log_cost  3.395808     l2   \n",
      "50  80.738363     79.850746          tanh  log_cost  0.013921     l2   \n",
      "68  84.456929     79.551821          relu  log_cost  0.313600   None   \n",
      "74  83.895131     79.551821       sigmoid  log_cost  0.494566     l2   \n",
      "9   69.662921     78.888889          relu  log_cost  0.012474     l2   \n",
      "..        ...           ...           ...       ...       ...    ...   \n",
      "59  62.279294     64.552239          tanh       mse  0.000640     l2   \n",
      "57  60.834671     63.432836       sigmoid  log_cost  0.000105     l2   \n",
      "58  60.834671     63.432836          tanh       mse  3.382727     l2   \n",
      "56  60.834671     63.432836          relu       mse  0.071297     l2   \n",
      "48  60.834671     63.432836          tanh       mse  5.212787   None   \n",
      "45  60.834671     63.432836       sigmoid  log_cost  0.000167     l2   \n",
      "42  60.834671     63.432836          relu       mse  0.004444     l2   \n",
      "31  61.657303     61.452514          relu  log_cost  0.017025     l2   \n",
      "24  61.657303     61.452514          relu       mse  0.000135   None   \n",
      "29  61.657303     61.452514       sigmoid       mse  0.000208     l2   \n",
      "25  61.657303     61.452514       sigmoid       mse  0.000367     l2   \n",
      "20  61.657303     61.452514          relu       mse  0.093488     l2   \n",
      "62  65.730337     61.344538          tanh       mse  2.410031     l2   \n",
      "76  63.670412     61.064426          relu       mse  0.008631     l2   \n",
      "34  61.376404     60.893855          relu       mse  0.001973     l2   \n",
      "78  63.483146     60.504202          tanh       mse  0.000150     l2   \n",
      "71  62.546816     60.224090          relu  log_cost  0.002868   None   \n",
      "69  62.546816     60.224090          relu  log_cost  6.956065     l2   \n",
      "67  62.546816     60.224090          relu  log_cost  5.122145   None   \n",
      "66  62.546816     60.224090          relu       mse  0.012246     l2   \n",
      "64  62.546816     60.224090          relu       mse  0.012647     l2   \n",
      "63  62.546816     60.224090       sigmoid  log_cost  0.001281     l2   \n",
      "61  62.546816     60.224090          relu  log_cost  0.000283     l2   \n",
      "75  56.179775     52.380952          relu  log_cost  0.000154     l2   \n",
      "55  43.178170     44.029851          relu  log_cost  0.000130     l2   \n",
      "77  37.453184     39.775910          relu       mse  1.248595     l2   \n",
      "37  38.342697     38.547486          relu       mse  0.080150     l2   \n",
      "41  39.165329     36.567164          relu       mse  0.272784   None   \n",
      "14  38.951311     33.333333       sigmoid       mse  8.571355     l2   \n",
      "8   38.951311     33.333333          relu  log_cost  2.353660   None   \n",
      "\n",
      "    reg_factor  test_size          layer_dims  \n",
      "11    2.194575        0.1     [7, 9, 9, 9, 1]  \n",
      "10    0.000088        0.1        [7, 3, 3, 1]  \n",
      "13    0.016207        0.1           [7, 7, 1]  \n",
      "1     0.092494        0.1  [7, 2, 2, 2, 2, 1]  \n",
      "17    0.543152        0.1        [7, 6, 6, 1]  \n",
      "5     3.520912        0.1     [7, 9, 9, 9, 1]  \n",
      "6     1.172512        0.1        [7, 6, 6, 1]  \n",
      "32    0.002851        0.2     [7, 8, 8, 8, 1]  \n",
      "35    0.038286        0.2        [7, 4, 4, 1]  \n",
      "19    4.149875        0.1  [7, 4, 4, 4, 4, 1]  \n",
      "27    0.002687        0.2        [7, 5, 5, 1]  \n",
      "46    0.009322        0.3  [7, 9, 9, 9, 9, 1]  \n",
      "53    1.026722        0.3           [7, 7, 1]  \n",
      "23    4.286436        0.2  [7, 2, 2, 2, 2, 1]  \n",
      "22    0.210048        0.2     [7, 2, 2, 2, 1]  \n",
      "39    0.000095        0.2        [7, 8, 8, 1]  \n",
      "28    0.000586        0.2           [7, 8, 1]  \n",
      "36    0.049654        0.2        [7, 8, 8, 1]  \n",
      "54    0.000043        0.3           [7, 6, 1]  \n",
      "0     0.048906        0.1  [7, 8, 8, 8, 8, 1]  \n",
      "38    0.000019        0.2  [7, 8, 8, 8, 8, 1]  \n",
      "26    0.184992        0.2     [7, 7, 7, 7, 1]  \n",
      "33    0.000026        0.2  [7, 6, 6, 6, 6, 1]  \n",
      "43    2.163369        0.3     [7, 5, 5, 5, 1]  \n",
      "47    0.000078        0.3  [7, 7, 7, 7, 7, 1]  \n",
      "44    0.000540        0.3        [7, 9, 9, 1]  \n",
      "50    0.000108        0.3           [7, 4, 1]  \n",
      "68    0.001644        0.4     [7, 4, 4, 4, 1]  \n",
      "74    0.008418        0.4           [7, 9, 1]  \n",
      "9     0.000023        0.1        [7, 5, 5, 1]  \n",
      "..         ...        ...                 ...  \n",
      "59    9.981029        0.3        [7, 7, 7, 1]  \n",
      "57    0.000064        0.3     [7, 6, 6, 6, 1]  \n",
      "58    0.081881        0.3     [7, 6, 6, 6, 1]  \n",
      "56    0.000023        0.3        [7, 3, 3, 1]  \n",
      "48    0.000847        0.3        [7, 8, 8, 1]  \n",
      "45    3.566415        0.3        [7, 7, 7, 1]  \n",
      "42    0.377774        0.3     [7, 9, 9, 9, 1]  \n",
      "31    6.050350        0.2  [7, 4, 4, 4, 4, 1]  \n",
      "24    0.003515        0.2  [7, 3, 3, 3, 3, 1]  \n",
      "29    0.948935        0.2        [7, 8, 8, 1]  \n",
      "25    0.000061        0.2        [7, 4, 4, 1]  \n",
      "20    1.935718        0.2     [7, 6, 6, 6, 1]  \n",
      "62    0.031473        0.4        [7, 7, 7, 1]  \n",
      "76    0.000137        0.4     [7, 9, 9, 9, 1]  \n",
      "34    0.605931        0.2           [7, 5, 1]  \n",
      "78    0.041344        0.4        [7, 5, 5, 1]  \n",
      "71    2.918591        0.4  [7, 4, 4, 4, 4, 1]  \n",
      "69    0.002399        0.4        [7, 4, 4, 1]  \n",
      "67    0.380472        0.4  [7, 2, 2, 2, 2, 1]  \n",
      "66    0.000056        0.4        [7, 6, 6, 1]  \n",
      "64    0.502117        0.4     [7, 9, 9, 9, 1]  \n",
      "63    3.267956        0.4     [7, 8, 8, 8, 1]  \n",
      "61    6.141744        0.4        [7, 2, 2, 1]  \n",
      "75    0.007750        0.4           [7, 3, 1]  \n",
      "55    0.000062        0.3     [7, 4, 4, 4, 1]  \n",
      "77    0.013416        0.4     [7, 2, 2, 2, 1]  \n",
      "37    0.730593        0.2     [7, 8, 8, 8, 1]  \n",
      "41    0.000120        0.3  [7, 5, 5, 5, 5, 1]  \n",
      "14    0.256939        0.1        [7, 4, 4, 1]  \n",
      "8     4.048862        0.1     [7, 6, 6, 6, 1]  \n",
      "\n",
      "[80 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    file_ = './saved/grid_data_4test_sizes_40samples.pickled'\n",
    "    with open(file_, 'rb') as fp:\n",
    "        grid_data = pickle.load(fp)\n",
    "    # grid_df = pd.read_csv('./saved/grid_data_4test_sizes_40samples.csv')\n",
    "    grid_df = pd.DataFrame(grid_data)\n",
    "    \n",
    "    sorted_grid = grid_df.sort_values('accuracy_dev', ascending=False)\n",
    "\n",
    "    if False:\n",
    "        metric = 'reg_factor'\n",
    "        x = np.arange(len(sorted_grid))\n",
    "        y = sorted_grid[metric]\n",
    "        plt.scatter(x,y, color='blue')\n",
    "        metric = None\n",
    "        if metric is not None:\n",
    "            y = sorted_grid[metric]\n",
    "            plt.plot(x,y, color='green')\n",
    "        plt.show()\n",
    "\n",
    "    relevant_cols = ['accuracy', 'accuracy_dev', 'activation_fn', 'cost_fn',\n",
    "                     'lrate', 'reg_fn', 'reg_factor', 'test_size', 'layer_dims']\n",
    "    print(sorted_grid[relevant_cols]) #[sorted_grid['test_size'] == 0.4])\n",
    "\n",
    "    # sorted_grid['num_layers'] = sorted_grid['layer_dims'].apply(lambda x: len(x)-2)\n",
    "    # sorted_grid['num_layers']\n",
    "    # sorted_grid['num_neurons'] = sorted_grid['layer_dims'].apply(lambda x: x[2])\n",
    "    # sorted_grid['num_neurons']\n",
    "\n",
    "    # transformed_grid = pd.DataFrame(sorted_grid.models.as_matrix().tolist())\n",
    "    # transformed_grid.head()\n",
    "\n",
    "    # print(sorted_grid.columns)\n",
    "\n",
    "    # for i, idx in enumerate(sorted_grid.index):\n",
    "    #     print('Model {}', i+1)\n",
    "    #     print('')\n",
    "    #     print(sorted_grid['models'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "793c7823-dbba-4d41-a935-dbb1690a7a27",
    "_uuid": "6c643dcddea81bf3e072e595dcbdf32ed44651e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions Saved\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    final_index = 11\n",
    "    final_model = get_model_config(**{\n",
    "        'activation_fn': grid_data[final_index]['activation_fn'],\n",
    "        'cost_fn': grid_data[final_index]['cost_fn'],\n",
    "        'layer_dims': grid_data[final_index]['layer_dims'],\n",
    "        'lrate': grid_data[final_index]['lrate'],\n",
    "        'niters': grid_data[final_index]['niters'],\n",
    "        'test_size': grid_data[final_index]['test_size'],\n",
    "        'reg_fn': grid_data[final_index]['reg_fn'],\n",
    "        'reg_factor': grid_data[final_index]['reg_factor']\n",
    "        })\n",
    "\n",
    "    final_weights = grid_data[final_index]['weights']\n",
    "    test_data, _ = preprocess(test_orig, grid_data[final_index]['norms'], test=True)\n",
    "    X_test = test_data.as_matrix().T\n",
    "if True:\n",
    "    output_dir = './'\n",
    "    sumbission_path = os.path.join(output_dir, './saved/gender_submission_hp_tuned_1.csv')\n",
    "    predictions_test = predict(X_test, final_weights, final_model)\n",
    "    submission_data = test_orig[['PassengerId']]\n",
    "    submission_data = submission_data.assign(Survived = pd.Series(predictions_test.reshape(-1)))\n",
    "    submission_data.to_csv(sumbission_path, index=False)\n",
    "    print('Predictions Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "0933a8a5-5b39-463c-8f7c-2e5116c3d175",
    "_uuid": "a96c12fc6183d2830f4436ccda098d469a2661a2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>897</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>898</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>903</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>907</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>911</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>914</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>915</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>916</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>918</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>919</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>921</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>1280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>1281</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>1282</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>1283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>1284</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>1285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>1286</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1287</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>1288</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1289</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>1290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>1291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>1292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>1293</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>1295</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>1296</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>1297</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>1298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>1299</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>1300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>1301</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1303</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1304</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0            892         0\n",
       "1            893         0\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         0\n",
       "5            897         0\n",
       "6            898         1\n",
       "7            899         0\n",
       "8            900         1\n",
       "9            901         0\n",
       "10           902         0\n",
       "11           903         0\n",
       "12           904         1\n",
       "13           905         0\n",
       "14           906         1\n",
       "15           907         1\n",
       "16           908         0\n",
       "17           909         0\n",
       "18           910         0\n",
       "19           911         1\n",
       "20           912         0\n",
       "21           913         0\n",
       "22           914         1\n",
       "23           915         1\n",
       "24           916         1\n",
       "25           917         0\n",
       "26           918         1\n",
       "27           919         0\n",
       "28           920         0\n",
       "29           921         0\n",
       "..           ...       ...\n",
       "388         1280         0\n",
       "389         1281         0\n",
       "390         1282         0\n",
       "391         1283         1\n",
       "392         1284         0\n",
       "393         1285         0\n",
       "394         1286         0\n",
       "395         1287         1\n",
       "396         1288         0\n",
       "397         1289         1\n",
       "398         1290         0\n",
       "399         1291         0\n",
       "400         1292         1\n",
       "401         1293         0\n",
       "402         1294         1\n",
       "403         1295         1\n",
       "404         1296         0\n",
       "405         1297         0\n",
       "406         1298         0\n",
       "407         1299         0\n",
       "408         1300         1\n",
       "409         1301         0\n",
       "410         1302         1\n",
       "411         1303         1\n",
       "412         1304         0\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         0\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = './saved'\n",
    "sumbission_path = os.path.join(data_dir, 'gender_submission_hp_tuned_1.csv')\n",
    "sub_data = pd.read_csv(sumbission_path)\n",
    "sub_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle3",
   "language": "python",
   "name": "kaggle3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
